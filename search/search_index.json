{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Una aproximaci\u00f3n a los contenedores \u00bfQu\u00e9 es un contenedor? La virtualizaci\u00f3n es un proceso mediante el cual un software es usado para crear una abstracci\u00f3n sobre unos recursos , dando la sensaci\u00f3n de que los elementos hardware se dividen en varias computadores virtuales . Existen dos tipos t\u00e9cnicas principales de virtualizaci\u00f3n: M\u00e1quinas virtuales . Contenedores (virtualizaci\u00f3n ligera). Definici\u00f3n Un contenedor es un l\u00edmite l\u00f3gico que se crea dentro de un sistema operativo proporcionado por el aislamiento de recursos hardware. La caracter\u00edstica principal es que en esta t\u00e9cnica se utilizan herramientas que proporciona el Linux Kernel (como cgroups y namespaces ). Diferencia entre MV y contenedores Diferencia infraestructura: VM y contenedor Un contenedor es una forma de virtualizaci\u00f3n ligera . Normalmente envuelve a un peque\u00f1o grupo de procesos . Los contenedores comparten el kernel con el host. Dentro del contendor se encuentra \u00fanicamente el c\u00f3digo, librer\u00edas y ejecutables estrictamente necesarios . Usos principales Microservicios : los contenedores son ligeros y envuelven servicios muy peque\u00f1os , lo que los hace muy aptos para su uso en microservicios. DevOps + CI/CD : facilita el ciclo \" build, test and deploy \". Cloud : los contenedores pueden funcionar de forma consistente en pr\u00e1cticamente cualquier lugar . Ventajas principales Rapidez y ligereza . Portabilidad e independencia de plataformas . Escalabilidad . Desventajas principales Seguridad : el aislamiento entre contenedores y el host es menor: puede provocar vulnerabilidades. Monitorizaci\u00f3n : existen varias capas que monitorizar aunque s\u00f3lo se tenga una aplicaci\u00f3n en un contenedor. Complejidad a gran escala . Historia de contenedores Chroot (1979) chroot es una llamada al sistema que permite cambiar el directorio raiz de un proceso y de sus hijos a un nuevo lugar dentro del sistema de archivos. Esto hace que un grupo de procesos tengan una visi\u00f3n limitada del almacenamiento del sistema. El principal problema de chroot es que los procesos con permisos de root se pueden saltar el aislamiento con facilidad. FreeBSD Jails (2000) Fue el primer \"pseudo-contenedor\" y el impulsor de las tecnolog\u00edas de los contenedores que existen hoy en d\u00eda. Su objetivo era \"confinar el root omnipotente\" , que da nombre al documento donde se presenta la herramienta. Las Jails dan uso a chroot a\u00f1adiendo nuevos mecanismos existentes en el sistema operativo. En cada jail los procesos pueden manipular \u00fanicamente los servicios y archivos a los que se le da acceso (por ello se le dio el nombre de jail : c\u00e1rcel en ingl\u00e9s, como si los procesos estuvieran en prisi\u00f3n). El administrador del sistema puede separar el sistema en varias celdas asignando un superusuario a cada una sin perder el control del sistema completo. Para mantener la seguridad deseada se desactivaron ciertas llamadas al sistema (para evitar, por ejemplo, el spoofing ), aunque esto impida utilizar ciertas instrucciones comunes como el comando ping . Por \u00faltimo, cada celda tiene sus propios UID y GID : un mismo usuario en una celda puede corresponderse con otro usuario en distinta celda. Solaris Zones (2004) Los creadores de esta tecnolog\u00eda no s\u00f3lo buscaban mantener la seguridad , sino tambi\u00e9n mejorar el uso de recursos a gran escala . De hecho, se pretend\u00eda dar soporte a aplicaciones comerciales potencialmente escalables. El objetivo era lograr que los administradores necesitaran pocos minutos para configurar y lanzar una nueva zone : el sistema se encargar\u00eda de crearla autom\u00e1ticamente a\u00f1adiendo los l\u00edmites en recursos compartidos que se consum\u00edan (inicialmente s\u00f3lo CPU). El administrador puede configurar las propiedades de las zones en tiempo real o mediante scripts. Existen dos tipos de zones : Global zone : es la zone por defecto que tiene control sobre todos los procesos. Siempre existe aunque no se haya creado ninguna manualmente. Non-global zone : son las zones configuradas desde la global. Algo a tener en cuenta es que las Zones se preocupan por mantener ciertas utilidades sin dejar a un lado la seguridad (por ejemplo, permite utilizar el comando ping a diferencia de las jails ). Cgroups y Namespaces (2006-2007) En el 2006 se propuso un framework que agrupa procesos y aprovecha los mecanismos de control existentes del kernel . El objetivo es que los usuarios se centren en el controlador de recursos y se abstraigan de c\u00f3mo los procesos son gestionados y monitorizados. Este mecanismo en un principio se llamaba process containers pero se cambi\u00f3 el nombre a cgroups para diferenciarlo de los contenedores que conocemos hoy en d\u00eda, que usan otros mecanismos a parte de los cgroups . Por otro lado, en el 2002 se cre\u00f3 el primer namespace , el mount namespace . Aunque fue m\u00e1s adelante a partir del a\u00f1o 2007 cuando se comenz\u00f3 a ver el potencial que pod\u00edan tener junto a otras funcionalidades como los cgroups , as\u00ed que se empezaron a desarrollar nuevos namespaces . En la actualidad existen ocho. LXC (2008) Los LXC se podr\u00edan considerar los primeros contenedores tal y como conocemos el concepto hoy en d\u00eda: fue la primera tecnolog\u00eda en aplicar los dos elementos principales de los contenedores: cgroups y namespaces . Permite al usuario comunicarse con las facilidades que ofrecen las funcionalidades del kernel mediante una interfaz en l\u00ednea de comandos . Sin embargo, no es una herramienta apropiada para realizar tareas de gesti\u00f3n de contenedores en un alto nivel. LMCTFY y Docker (2013) LMCTFY ( Let Me Containerize That For You ) fue la versi\u00f3n open-source del stack de Google a los contenedores de Linux. Su desarrollo finaliz\u00f3 en 2015 y Google comenz\u00f3 a transferir parte de la implementaci\u00f3n a libcontainer ahora mismo es una parte fundamental del stack de Docker y forma parte de OCI. Por otro lado, Docker supuso un despunte en la popularidad de los contenedores en el a\u00f1o de su salida y el crecimiento de ambos conceptos han ido de la mano desde entonces siendo hoy en d\u00eda l\u00edder en este \u00e1mbito . En sus inicios utilizaba LXC, pero m\u00e1s tarde lo sustituy\u00f3 por su propia librer\u00eda libcontainer . OCI (2015) OCI ( Open Container Initiative ) es un proyecto de la Linux Foundation cuyo objetivo es dise\u00f1ar un est\u00e1ndar abierto para la virtualizaci\u00f3n basada en contenedores . Fue establecida en 2015 por Docker y otros l\u00edderes de la industria. Despu\u00e9s del lanzamiento de Docker, surgi\u00f3 una comunidad alrededor de los contenedores. Sin embargo, con el paso del tiempo fueron apareciendo nuevas tecnolog\u00edas y herramientas que satisfac\u00edan las neceseidades que iban surgiendo. Este fue el motivo principal por el que surgi\u00f3 este est\u00e1ndar. Actualmente, OCI define dos especificaciones, aunque hablaremos m\u00e1s tarde de ellas en este documento . Otras tecnolog\u00edas A lo largo de las dos \u00faltimas d\u00e9cadas han ido surgiendo otras tecnolog\u00edas de virtualizaci\u00f3n de sistema operativo, pero han sido menos importantes para el ecosistema de los contenedores que las anteriormente mencionadas. Entre ellas podemos encontrar a Linux VServer (2001), OpenVZ (2005), Warden (2011), Singularity (2015) o Podman (2018). Arquitectura de los contenedores Dada la ambig\u00fcedad con la que muchos profesionales se refieren a cada una de las capas que forman esta arquitectura y, dado que, dependiendo de la tecnolog\u00eda a usar, pueden cambiar ligeramente las funciones que realizan sus componentes, se utilizar\u00e1 como referencia el stack de Docker. Generalizaci\u00f3n de la arquitectura de los contenedores a partir de la arquitectura de Docker Comenzaremos el estudio por la parte inferior de la im\u00e1gen, los componentes del Linux Kernel, subiendo hasta llegar al Container Engine . 1\u00ba Componentes del Linux Kernel Existen numerosos componentes y herramientas del kernel que utilizan los contenedores para aislar los procesos. En concreto aqu\u00ed estudiaremos cinco de ellos, siendo los m\u00e1s importantes los dos primeros: Namespaces Definici\u00f3n de namespaces Proporcionan el aislamiento entre procesos mediante la encapsulaci\u00f3n de ciertos recursos del sistema. De esta forma, hacen creer a los procesos dentro de un contenedor que tienen su propia instancia del recurso, aunque realmente lo est\u00e1n compartiendo. Esta herramienta impide que, mediante una vulnerabilidad en un contenedor, unos intrusos puedan acceder a la m\u00e1quina completa comprometi\u00e9ndola. La API del kernel que facilita esta caracter\u00edstica tiene tres llamadas principales: Llamada Descripci\u00f3n clone Crea un proceso hijo al igual que fork pero proporcionando m\u00e1s control sobre qu\u00e9 partes del contexto se comparten. unshare Crea un nuevo namespace y ejecuta un nuevo proceso dentro de \u00e9ste. setns Mueve el hilo actual dentro de un namespace existente ioctl Ofrece informaci\u00f3n sobre namespaces (la informaci\u00f3n se maneja desde /proc/$PID/ns ) En la actualidad existen 8 namespaces distintos, cada uno de ellos proporciona un distinto tipo de aislamiento : Tipos de namespaces MNT NS Mount Namespace Fue el primer namespace implementado. En aquel momento no se sab\u00eda que iban a crearse m\u00e1s tipos de namespaces as\u00ed que su flag correspondiente en los comandos de clone y unshare es CLONE_NEWNS . Funci\u00f3n Proporcionan aislamiento a las estructuras de datos que utiliza el sistema para gestionar los puntos de montaje . De esta forma, los procesos en distinto mount namespace tienen una visi\u00f3n distinta de la jerarqu\u00eda de los sistemas de archivos. Algunos usos que ofrece son: Cada usuario puede tener su propio /tmp para aumentar la seguridad frente a un usuario malicioso. Distintos procesos pueden tener un sistema de archivos ra\u00edz (es un concepto parecido a chroot ). Los puntos de montaje pueden ser privados o compartidos. UTS NS UNIX Time-Sharing Desde la implementaci\u00f3n del anterior namespace hasta este pasaron 4 a\u00f1os. Este es el m\u00e1s sencillo. Funci\u00f3n A\u00edsla el hostname y el domain-name del contenedor. La flag correspondiente es CLONE_NEWUTS . IPC NS Inter Process Communication Funci\u00f3n A\u00edsla cierta la compartici\u00f3n de ciertos recursos que facilitan la comunicaci\u00f3n como la memoria, objetos System V IPC y colas de mensajes POSIX. Sin embargo, despu\u00e9s de un clone se siguen compartiendo se\u00f1ales, sockets, descriptores de archivos o sondeos de memoria (habr\u00eda que aislarlos con otros namespaces ). Su flag correspondiente es CLONE_NETIPC . PID NS Process ID Los procesos de un sistema pertenecen a un \u00e1rbol de procesos global visible \u00fanicamente por el host . Funci\u00f3n Este namespace crea un nuevo \u00e1rbol de procesos propio de cada contenedor. Los procesos que est\u00e9n dentro de este namespace pertenecen , por tanto, a dos \u00e1rboles , el global y el propio del namespace . De esta forma un mismo proceso puede tener varios PIDs dependiendo de en qu\u00e9 \u00e1rbol se consulte. Distintos \u00e1rboles para distintos namespaces . El primer PID dentro del nuevo namespace siempre es el 1. Este proceso deber\u00eda tener unas caracter\u00edsticas \u00fanicas para poder funcionar como init y as\u00ed poder tener m\u00e1s de un proceso en un contenedor. Uno de los principales beneficios que presenta es que los contenedores se pueden migrar de una m\u00e1quina a otra manteniendo el \u00e1rbol de procesos. Su flag es CLONE_NEWPID . NET NS Network Funci\u00f3n Proporciona aislamiento de los recursos de red , donde cada contenedor puede tener su propio stack de recursos: tablas de enrutado, reglas de iptables , sockets , etc. En las tecnolog\u00edas actuales de contenedores se suele utilizar el modelo bridge . En este modelo el puente, que est\u00e1 en el namespace global, ofrece la conexi\u00f3n a los contenedores mediante una interfaz virtual. A la vez, cada contenedor, en su propio namespace , tiene una interfaz virtual conectada a la del puente. Ejemplo generalizado de bridge. La flag correspondiente es CLONE_NEWNET . User NS Funci\u00f3n Permite que los procesos puedan creer que est\u00e1n operando como root dentro de un contenedor, pero fuera del contenedor tienen realmente los privilegios de un usuario com\u00fan. Antes de que existiera este namespace si un proceso ten\u00eda permisos de root en cualquier entorno de aislamiento, tambi\u00e9n lo ten\u00eda en el sistema global, lo que era un problema muy grande en lo que a la seguridad se refiere. En este namespace el UID dentro del namespace siempre se corresponder\u00e1 con otro fuera de \u00e9l en el host. Este mapeo se puede hacer manualmente y elegir qu\u00e9 UID se selecciona dentro del contenedor para un usuario global. Un ejemplo de mapeo entre las dos tablas de usuarios se puede observar en la siguiente imagen. Ejemplo de mapeo entre un UID del host con un UID dentro del contenedor La flag correspondiente es CLONE_NETUSER . Cgroup NS Los cgroups (rupos de control en espa\u00f1ol) son una funcionalidad del kernel de Linux que trataremos en la secci\u00f3n que, b\u00e1sicamente, implementan monitorizaci\u00f3n y limitaci\u00f3n de los recursos que consume un grupo de procesos. Funci\u00f3n Virtualiza la vista de los grupos de control que ven los procesos. Sin esta restricci\u00f3n, un proceso podr\u00eda observar los grupos de control globales y acceder a la informaci\u00f3n sobre la limitaci\u00f3n de recursos de otros procesos. Tiene dos funciones principales: Evita que un grupo de control acceda a los l\u00edmites de otro superior a \u00e9l. Facilita la migraci\u00f3n de contenedores al aislar unos grupos de otros. De esta forma podemos ahorrar la necesidad de replicar los l\u00edmites seg\u00fan se van trasladando los contenedores. La flag correspondiente es CLONE_NEWCGROUP . Time NS Funci\u00f3n Virtualiza dos relojes del sistema permitiendo que, cuando un contenedor se migra de un nodo a otro, estos relojes son restaurados de forma consistente partiendo siempre del tiempo que ten\u00eda el reloj antes de ser migrado. Estos dos relojes son el CLOCK_MONOTONIC y CLOCK_BOOTTIME . La flag correspondiente es CLONE_NEWTIME . Control Groups Antes de que se crearan los grupos de control exist\u00edan formas para monitorizar y controlar procesos individuales , pero no hab\u00eda soporte para aplicar esas mismas operaciones a grupos de procesos . Por eso se acabaron implementando los Process Containers , que luego se renombraron a cgroups . Definici\u00f3n de Cgroups Son un framework basado en los mecanismos existentes del kernel para proporcionar una interfaz y un alcance m\u00e1s global a las operaciones de control y monitorizaci\u00f3n de procesos. Los grupos de control permiten repartir y asignar recursos entre un grupo de tareas o procesos del sistema. Los grupos de control ofrecen cuatro caracter\u00edsticas principales: Limitaci\u00f3n de recursos , como procesador, memoria, dispositivos E/S, etc. Priorizaci\u00f3n , que es parecida a la anterior caracter\u00edstica, pero no limita recursos a procesos sino que le da preferencia en el consumo de recursos a un proceso seleccionado. Monitorizaci\u00f3n de un grupo de procesos para obtener informaci\u00f3n de qu\u00e9 procesos est\u00e1n consumiendo cu\u00e1ntos recursos. Freezing de procesos . Esta es una herramienta que permite paralizar y retomar grupos de procesos. Es muy utilizada en el procesamiento por lotes. Existen dos versiones de Cgroups , hoy en d\u00eda se est\u00e1 intentando hacer la transici\u00f3n en la mayor\u00eda de herramientas a la segunda versi\u00f3n. Para comprender esta \u00faltima versi\u00f3n necesitamos entender tres conceptos: Cgroup : es un grupo de tareas al que se le asocia uno o m\u00e1s subsistemas. Subsistema o resource controller : representa un \u00fanico recurso, como la memoria o el tiempo de CPU. Jerarqu\u00eda : es el conjunto de todos los cgroups en forma de \u00e1rbol. Cada proceso del sistema est\u00e1 en un cgroup determinado. Cada nodo (o sea, cada cgroup ) del \u00e1rbol tiene asociados uno o m\u00e1s subsistemas. En la versi\u00f3n 2 de los cgroups existe una s\u00f3la jerarqu\u00eda. En la siguiente imagen podemos observar un ejemplo de jerarqu\u00eda, donde existen cuatro cgroups distintos. Cada uno de ellos tiene asociados varios subsistemas. En el caso del Group_2 los subsistemas asociados son \"memoria\" y \"pids\", y sus grupos hijos heredan los mismos subsistemas, pero pueden asociar otros nuevos o disociar los heredados. Ejemplo de jerarqu\u00eda de grupos de control. Cada grupo se a\u00f1ade creando un nuevo directorio dentro de la carpeta ra\u00edz /sys/fs/cgroup . En el momento de crear dicha carpeta, el sistema a\u00f1ade los archivos necesarios para gestionar los cgroups . \u00bfC\u00f3mo a\u00f1adimos los grupos de control de este ejemplo? Los subsistemas del siguiente ejemplo se a\u00f1adir\u00edan creando en primer lugar cada cgroup y m\u00e1s tarde escribiendo en los subsistemas en los archivos autogenerados: mkdir /sys/fs/cgroup/Cgroup_1 mkdir /sys/fs/cgroup/Cgroup_2 mkdir /sys/fs/cgroup/Cgroup_2.1 mkdir /sys/fs/cgroup/Cgroup_2.2 echo \"+memory +io\" > /sys/fs/cgroup/Group_1/cgroup.subtree_control echo \"+memory +pids\" > /sys/fs/cgroup/Group_2/cgroup.subtree_control echo \"+io\" > /sys/fs/cgroup/Group_2.1/cgroup.subtree_control echo \"-pids\" > /sys/fs/cgroup/Group_2.2/cgroup.subtree_control Union Filesystem Los sistemas de archivos por capas permiten compartir archivos en el disco, lo que supone un ahorro de espacio. Definici\u00f3n Los Union Filesystem son un tipo de sistema de archivos por capas. Permite a los archivos y directorios de sistemas de archivos distintos estar superpuestos formando un \u00fanico sistema de archivos . OverlayFS Docker, actualmente, utiliza un tipo de Union Filesystem llamado Overlay2. Esta tecnolog\u00eda utiliza tres capas: Base (s\u00f3lo lectura): es la capa donde van los archivos base. En terminolog\u00eda de Docker se corresponder\u00eda con la imagen. Overlay : es la capa donde opera el usuario. En un principio ofrece una vista de la capa base y permite operar sobre ella, aunque los cambios no se guardan en esta capa. Diff : es la capa donde se guardan los cambios realizados en la capa anterior. Una de las caracter\u00edsticas fundamentales que usan los UnionFS es la t\u00e9cnica de COW ( Copy On Write ) que permite reducir el consumo de copias sin modificar . Si un fichero existe en la capa base y otra capa quiere leerlo, lee el original. Cuando una capa quiera modificar el archivo es cuando debe copiarla y modificarla en la propia capa que la necesite. Las tres capas de Overlay2 junto a la t\u00e9cnica COW. Capabilities El usuario con UID 0 es el root y tiene el control completo del sistema. Esto puede traer problemas recurrentes de seguridad (las Jails ya intentaron confinar el root omnipotente). Por eso, las capabilities proponen una soluci\u00f3n. Definici\u00f3n Las capabilities dividen los permisos que tiene un usuario en varias particiones (cada una de ellas es una capability ). Un buen ejemplo de su uso podr\u00eda ser la posibilidad de permitir a un binario poder crear un raw socket como en el caso de la instrucci\u00f3n ping mediante la capability CAP_NET_RAW sin necesidad de asignarle el resto de privilegios que ofrece el usuario root. Existen dos tipos de capabilities , unas son las asociadas a procesos y otros asociadas a archivos. Las que est\u00e1n asociadas a archivos se unen con las asociadas a procesos cuando un proceso quiere ejecutar un archivo. En la pr\u00e1ctica, los contenedores no necesitan todos los privilegios que ofrece el root. As\u00ed que, realmente, los usuarios root dentro de los contenedores tienen asignadas algunas capabilities para ofrecer \u00fanicamente ciertos permisos y restringir otros potencialmente inseguros . Pivot_root En la secci\u00f3n Historia de los contenedores ya explicamos la importancia que tuvo chroot para aparentar que cambiaba el directorio ra\u00edz de un proceso y de sus hijos. El problema que presenta es que no es segura, as\u00ed que es mejor utilizar la alternativa pivot_root . La funci\u00f3n completa de pivot_root es la siguiente: int pivot_root ( const char *new_root, const char *put_old ) ; Pivot_root mueve el punto de montaje ra\u00edz al directorio put_old haci\u00e9ndolo inaccesible para el proceso llamador como a todos sus hijos, mientras que convierte a new_root en el nuevo punto de montaje ra\u00edz . Esto soluciona las brechas de seguridad de chroot porque se aplica al mount namespace , as\u00ed que est\u00e1 cambiando \u00fanicamente en el namespace del contenedor la posibilidad de acceder a la antigua ra\u00edz. 2\u00ba Container Runtimes \u00bfQu\u00e9 es un container runtime? Un container runtime es la herramienta o capa responsable de que el contenedor se ejecute correctamente (sin incluir los procesos que est\u00e1n dentro del contenedor). Este t\u00e9rmino puede tener diferentes significados dependiendo del proyecto o comunidad donde se consulte, principalmente debido a que el rango de tareas que realiza un runtime no est\u00e1 plenamente definido. En este documento vamos a separar los container runtimes seg\u00fan las tareas que realicen en low-level y high-level . Algunos ejemplos de low-level pueden ser runc, crun, gvisor o kata-runtime , mientras que entre los ejemplos de high-level podemos encontrarnos con containerd o CRI-O . \u00bfY Docker? \u00bfD\u00f3nde lo metemos? En muchas referencias se trata a Docker como un container runtime (refiri\u00e9ndose a Docker por su significado de daemon m\u00e1s su CLI). Llegados a este punto podemos entender perfectamente que se puede considerar de esta forma, ya que su funci\u00f3n es gestionar los contenedores y las im\u00e1genes (definici\u00f3n de high-level container runtime ). Sin embargo, en este documento se va a colocar en un escal\u00f3n superior, en los container engines . Esta decisi\u00f3n se ha tomado en base a dos razones: la primera es que una de las caracter\u00edsticas principales que hacen a esta herramienta m\u00e1s popular que containerd o CRI-O entre los usuarios es su capacidad de facilitar la comunicaci\u00f3n con el usuario. La segunda raz\u00f3n es que delega gran parte del trabajo en otro high-level runtime , que por defecto es containerd. Dicho esto, reiteramos que podr\u00eda ser perfectamente considerado un container runtime . De la misma forma, CRI-O o containerd se podr\u00edan llegar a considerar container engines si se utilizan a trav\u00e9s de plugins CLI que permitan al usuario interactuar debidamente. En resumen, la forma de referirse a estas tecnolog\u00edas no es universal y, como hemos dicho, dependiendo de la comunidad y el proyecto, las etiquetas pueden cambiar . Una forma de entender gr\u00e1ficamente el porqu\u00e9 de que estos t\u00e9rminos sean tan ambiguos se puede observar en la siguiente figura, que muestra de forma muy general y subjetiva el nivel al que operan algunos ejemplos de container runtimes . Nivel al que opera cada container runtime Low-level container runtimes En esta p\u00e1gina nos referiremos a los low-level runtimes a las capas que proporcionan las utilidades b\u00e1sicas como crear namespaces y comenzar el proceso de encapsular una aplicaci\u00f3n en un contenedor, es decir a las herramientas que se comunican directamente con el kernel . runc (parte del stack de Docker) Definici\u00f3n Runc es un CLI que se encarga de crear contenedores y ejecutarlos seg\u00fan la especificaci\u00f3n que se le proporcione. Esta especificaci\u00f3n sigue el est\u00e1ndar OCI. La raz\u00f3n de que sea un low-level container runtime cuando realmente s\u00f3lo es un CLI es que antiguamente Docker utilizaba como container runtime LXC. Esto cambi\u00f3 m\u00e1s adelante y comenz\u00f3 a crear junto con Google una librer\u00eda propia que sustituyera a LXC: libcontainer . Hoy en d\u00eda, runc utiliza libcontainer , de hecho es su pieza b\u00e1sica y fundamental, por eso se pueden considerar que ambos son la misma capa. \u00bfQu\u00e9 est\u00e1ndar OCI? Los est\u00e1ndares OCI se introdujeron en la secci\u00f3n Historia de los contenedores . Consta de dos especificaciones: Imagen : define el formato del archivo de las im\u00e1genes OCI. El objetivo es permitir una serie de herramientas estandarizadas que puedan preparar, construir y transportar una imagen. Runtime : define la configuraci\u00f3n, entorno de ejecuci\u00f3n y ciclo de vida de un contenedor. Runc s\u00f3lo implementa la segunda especificaci\u00f3n ya que no entiende de im\u00e1genes, sino de bundles . Un bundle es un conjunto de archivos que contiene todos los datos necesarios para que un runtime pueda realizar todas las operaciones necesarias para crear y ejecutar un contenedor . crun Se trata de otro low-level container runtime con funciones similares a runc. La diferencia principal radica en que \u00e9ste est\u00e1 escrito en C, mientras que runc est\u00e1 escrito en Go. Seg\u00fan el creador de crun, Go es un lenguaje multihilo por defecto, lo que no es recomendable para utilizar en un modelo fork/exec . Adem\u00e1s, utiliza los cgroups v2 , algo que no todas las herramientas de este \u00e1mbito soportan. Otros Sandboxed Runtimes Las anteriores tecnolog\u00edas comparten sistema operativo con el host. Los Sandboxed runtimes son otro tipo de herramientas que aumentan el aislamiento con el host (aunque disminuyendo la eficiencia), lo que es recomendable en entornos donde varios clientes compartan las mismas m\u00e1quinas f\u00edsicas. Esto se logra separando los sistemas operativos del host y de los contenedores. Algunos ejemplos son gVisor (de Google), Firecracker (de Amazon) o Kata (de OpenStack). Comparaci\u00f3n gen\u00e9rica de Sandboxed Runtime y runc High-level container runtime En esta p\u00e1gina nos referimos a los high-level container runtimes a las capas que son responsables de la gesti\u00f3n y transporte de las im\u00e1genes de los contenedores, as\u00ed como de desempaquetarlas y enviarlas a los low-level container runtimes abstray\u00e9ndolos de esas actividades. Containerd (parte del stack de Docker) Es un est\u00e1ndar en la industria dada su portabilidad y facilidad de uso . Implementa ambas especificaciones OCI : Imagen : es capaz de desempaquetar im\u00e1genes OCI y de crear bundles a partir de ellas que el low-level puede entender. Runtime : puede comunicarse con un low-level que tambi\u00e9n implemente el est\u00e1ndar OCI y delegar el trabajo restante en \u00e9l. Tareas principales Hacer pull y push a las im\u00e1genes. Gestionar el almacenamiento de contenedores. Gestionar las redes y sus interfaces. Llamar al low-level container runtime con los par\u00e1metros deseados. Cabe destacar que containerd es un daemon , aunque dispone de varios plugins que aumentan sus funcionalidades en gran medida como una interfaz en l\u00ednea de comandos. Containerd-shim Si recordamos las capas que forman el stack de Docker, encontramos una pieza llamada containerd-shim que no se corresponde con ning\u00fan componente generalizado. Containerd-shim Containerd-shim es una implementaci\u00f3n que permite separar a los low-level container runtimes de containerd Cuando containerd quiere crear un contenedor, llama a runc creando una nueva instancia de runc por cada contenedor. Cuando runc termina la tarea de creaci\u00f3n se termina a si mismo. A partir de ahora es containerd-shim la pieza que se encarga de tomar las responsabilidades de ser padre del contenedor. Containerd-shim despu\u00e9s de que runc acabe su tarea CRI-O CRI (Container Runtime Interface) CRI ( Container Runtime Interface ) es la API que utiliza Kubernetes para hablar con los high-level container runtimes . El proyecto parti\u00f3 del hecho de que Kubernetes es un orquestrador que delega parte del trabajo en los runtimes . Si se establece una API (un est\u00e1ndar al fin y al cabo) que describa c\u00f3mo deben interactuar estas tecnolog\u00edas con Kubernetes se consigue aumentar en gran medida la interoperabilidad. CRI est\u00e1 basado en gRPC , que es un tipo de Remote Procedure Call desarrollado por Google que permite intercomunicaci\u00f3n entre varios lenguajes. Ahora ya podemos entender mejor qu\u00e9 es CRI-O y por qu\u00e9 se llama as\u00ed. Su desarrollo lo llev\u00f3 Red Hat con el objetivo de crear un runtime que soportara de forma nativa la comunicaci\u00f3n mediante CRI, sirviendo de puente entre Kubernetes y un low-level container runtime compatible con OCI. Comparaci\u00f3n con containerd En primer lugar, cabe destacar que existe un plugin de containerd que permite la comunicaci\u00f3n con Kubernetes mediante CRI. Un estudio encabezado por Lennart Espe en el que eval\u00faa el rendimiento de CRI-O y containerd utilizando como low-level tanto a runc como gVisor muestra que, en cuanto a rendimiento general , CRI-O m\u00e1s runc resulta ser la mejor opci\u00f3n. Cabe destacar que CRI-O est\u00e1 muy unido a Kubernetes y la gesti\u00f3n de las im\u00e1genes es recomendable encomend\u00e1rsela a otro container runtime o container engine como Podman (que es de Red Hat al igual que CRI-O). 3\u00ba Container engines El container engine por excelencia es el propio Docker, formado por Docker daemon m\u00e1s sus APIs y CLI: Componentes Docker daemon Es el servicio que se ejecuta en la parte del host o servidor y escucha peticiones API. Es la herramienta que comienza la creaci\u00f3n del contenedor. API REST El deamon escicha las peticiones de los clientes a trav\u00e9s de esta API. CLI Es la forma con la que los usuarios, actuando como clientes, se comunican con el daemon a trav\u00e9s de la API. 4\u00ba Otros componentes Otros componentes que forman el entorno de los contenedores son las im\u00e1genes y los registros . Im\u00e1genes Una imagen es un archivo binario inmutable que contiene el c\u00f3digo, librer\u00edas, dependencias e informaci\u00f3n que necesita un runtime para crear un contenedor. El hecho de que sean archivos inmutables permite que equipos de desarrollo puedan probar software en un entorno estable y uniforme ya que representan un entorno en un momento y condiciones espec\u00edficas . En el caso particular de Docker existe un archivo llamado Dockerfile , que es un fichero de texto que contiene una lista de instrucciones que se van a ejecutar a la hora de crear la imagen, es decir, cuando se ejecute el comando docker build . El Dockerfile, habitualmente, utiliza como base otra imagen anterior y se le a\u00f1aden nuevas instrucciones, as\u00ed, una imagen se crea normalmente mediante el apilamiento de varias im\u00e1genes base. Registros Un registro es un sistema o repositorio que se encarga de almacenar y distribuir im\u00e1genes . Existen dos tipos de repositorios: P\u00fablicos : como Docker Hub. Ideal para equipos peque\u00f1os o usuarios que buscan una imagen que cumpla una funci\u00f3n espec\u00edfica sin importarles demasiado la privacidad y seguridad. Privados : s\u00f3lo se comparten con los usuarios deseados, normalmente, un equipo de trabajo. En la siguiente imagen podemos ver todos los componentes explicados interactuar entre s\u00ed. Todos los componentes explicados de Docker interactuando entre s\u00ed. 4\u00ba Orquestadores Administrar un gran n\u00famero de contenedores es una tarea muy complicada, por eso surgieron ciertas tecnolog\u00edas que permiten automatizar el proceso de crear, desplegar y escalar contenedores . Estas tecnolog\u00edas son los orquestadores. Su principal objetivo es facilitar la gesti\u00f3n de contenedores localizados en muchos hosts. Esto, adem\u00e1s, facilita a los equipos de DevOps integrar sus flujos CI/CD. Tareas de los orquestadores Configurar y programar contenedores. Proporcionar el aprovisionamiento y despliegue adecuados de los contenedores. Asegurar la disponibilidad de los servicios. Escalar los contenedores. Controlar los recursos compartidos. Balancear la carga y el tr\u00e1fico. Monitorizar la salud de los contenedores. Proporcionar seguridad en las interacciones entre los contenedores. Kubernetes Kubernetes es el est\u00e1ndar de facto en la actualidad en cuando a lo que orquestadores se refiere. Es una herramienta portable y de c\u00f3digo libre desarrollado originalmente por Google. Sus tres principios son seguridad, facilidad de uso y extensibilidad . Cuando se despliega una instancia de Kubernetes se est\u00e1 creando realmente un cluster . Este cluster tiene dos elementos fundamentales: control plane y compute machines o nodos : Componentes de un cluster de Kubernetes Control Plane API Server : maneja las peticiones internas y externas. El CLI kubelet que se suele usar se conecta a este componente. Scheduler : controla la salud de los pods y puede a\u00f1adir o quitar alguno. Controller Manager : controla que el cluster funciona correctamente (realmente est\u00e1 formado por varios controladores, cada uno con una funci\u00f3n espec\u00edfica). etcd : es una base de datos clave-valor distribuida. Permite almacenar la configuraci\u00f3n y el estado de los clusters . Nodos Pods : representa una instancia de una aplicaci\u00f3n, se corresponde con un contenedor o varios altamente acoplados. Container runtime/engine : se pueden utilizar varios como containerd, CRI-O o Docker. Kubelet : es la aplicaci\u00f3n que controla a los contenedores de un mismo pod . Proxy : facilita los servicios de red. Docker-shim Hab\u00edamos dicho que Kubernetes utiliza un CRI para comunicarse con los container engines y container runtimes . Como Docker no implementa CRI, la comunidad de Kubernetes cre\u00f3 una herramienta que permite comunicarse con Docker a trav\u00e9s de CRI. Sin embargo, en el a\u00f1o 2020 la comunidad de Kubernetes anunci\u00f3n que no va a seguir manteniendo este plugin, as\u00ed que el uso de Kubernetes m\u00e1s Docker tiene un futuro incierto. Docker-shim haciendo de puente entre Kubernetes y Docker. Kubernetes y sus Container runtimes Como hemos visto hasta ahora existen muchas opciones a la hora de elegir container runtime para Kubernetes. Cada opci\u00f3n tiene sus pros y sus contras en la pr\u00e1ctica, sin embargo, si analizamos las capas que estar\u00edamos a\u00f1adiendo en cada opci\u00f3n hay ciertas diferencias que merece la pena destacar. Todos los componentes explicados de Docker interactuando entre s\u00ed. Como se puede comprobar en la \u00faltima imagen, dependiendo de la opci\u00f3n que escojamos, vamos a encontrarnos m\u00e1s componentes o menos en el stack. Esto no quiere decir que los stacks con menos saltos sean m\u00e1s eficientes: cada alternativa tiene sus pros y sus contras que habr\u00eda que estudiar con mayor detenimiento. AMPLIACI\u00d3N DE CONTENIDOS Los contenidos explicados en esta pagina han sido obtenidos \u00edntegramente de * ENLACE A LA MEMORIA EN GITHUB *","title":"Home"},{"location":"#una-aproximacion-a-los-contenedores","text":"","title":"Una aproximaci\u00f3n a los contenedores"},{"location":"#que-es-un-contenedor","text":"La virtualizaci\u00f3n es un proceso mediante el cual un software es usado para crear una abstracci\u00f3n sobre unos recursos , dando la sensaci\u00f3n de que los elementos hardware se dividen en varias computadores virtuales . Existen dos tipos t\u00e9cnicas principales de virtualizaci\u00f3n: M\u00e1quinas virtuales . Contenedores (virtualizaci\u00f3n ligera). Definici\u00f3n Un contenedor es un l\u00edmite l\u00f3gico que se crea dentro de un sistema operativo proporcionado por el aislamiento de recursos hardware. La caracter\u00edstica principal es que en esta t\u00e9cnica se utilizan herramientas que proporciona el Linux Kernel (como cgroups y namespaces ).","title":"\u00bfQu\u00e9 es un contenedor?"},{"location":"#diferencia-entre-mv-y-contenedores","text":"Diferencia infraestructura: VM y contenedor Un contenedor es una forma de virtualizaci\u00f3n ligera . Normalmente envuelve a un peque\u00f1o grupo de procesos . Los contenedores comparten el kernel con el host. Dentro del contendor se encuentra \u00fanicamente el c\u00f3digo, librer\u00edas y ejecutables estrictamente necesarios . Usos principales Microservicios : los contenedores son ligeros y envuelven servicios muy peque\u00f1os , lo que los hace muy aptos para su uso en microservicios. DevOps + CI/CD : facilita el ciclo \" build, test and deploy \". Cloud : los contenedores pueden funcionar de forma consistente en pr\u00e1cticamente cualquier lugar . Ventajas principales Rapidez y ligereza . Portabilidad e independencia de plataformas . Escalabilidad . Desventajas principales Seguridad : el aislamiento entre contenedores y el host es menor: puede provocar vulnerabilidades. Monitorizaci\u00f3n : existen varias capas que monitorizar aunque s\u00f3lo se tenga una aplicaci\u00f3n en un contenedor. Complejidad a gran escala .","title":"Diferencia entre MV y contenedores"},{"location":"#historia-de-contenedores","text":"","title":"Historia de contenedores"},{"location":"#chroot-1979","text":"chroot es una llamada al sistema que permite cambiar el directorio raiz de un proceso y de sus hijos a un nuevo lugar dentro del sistema de archivos. Esto hace que un grupo de procesos tengan una visi\u00f3n limitada del almacenamiento del sistema. El principal problema de chroot es que los procesos con permisos de root se pueden saltar el aislamiento con facilidad.","title":"Chroot (1979)"},{"location":"#freebsd-jails-2000","text":"Fue el primer \"pseudo-contenedor\" y el impulsor de las tecnolog\u00edas de los contenedores que existen hoy en d\u00eda. Su objetivo era \"confinar el root omnipotente\" , que da nombre al documento donde se presenta la herramienta. Las Jails dan uso a chroot a\u00f1adiendo nuevos mecanismos existentes en el sistema operativo. En cada jail los procesos pueden manipular \u00fanicamente los servicios y archivos a los que se le da acceso (por ello se le dio el nombre de jail : c\u00e1rcel en ingl\u00e9s, como si los procesos estuvieran en prisi\u00f3n). El administrador del sistema puede separar el sistema en varias celdas asignando un superusuario a cada una sin perder el control del sistema completo. Para mantener la seguridad deseada se desactivaron ciertas llamadas al sistema (para evitar, por ejemplo, el spoofing ), aunque esto impida utilizar ciertas instrucciones comunes como el comando ping . Por \u00faltimo, cada celda tiene sus propios UID y GID : un mismo usuario en una celda puede corresponderse con otro usuario en distinta celda.","title":"FreeBSD Jails (2000)"},{"location":"#solaris-zones-2004","text":"Los creadores de esta tecnolog\u00eda no s\u00f3lo buscaban mantener la seguridad , sino tambi\u00e9n mejorar el uso de recursos a gran escala . De hecho, se pretend\u00eda dar soporte a aplicaciones comerciales potencialmente escalables. El objetivo era lograr que los administradores necesitaran pocos minutos para configurar y lanzar una nueva zone : el sistema se encargar\u00eda de crearla autom\u00e1ticamente a\u00f1adiendo los l\u00edmites en recursos compartidos que se consum\u00edan (inicialmente s\u00f3lo CPU). El administrador puede configurar las propiedades de las zones en tiempo real o mediante scripts. Existen dos tipos de zones : Global zone : es la zone por defecto que tiene control sobre todos los procesos. Siempre existe aunque no se haya creado ninguna manualmente. Non-global zone : son las zones configuradas desde la global. Algo a tener en cuenta es que las Zones se preocupan por mantener ciertas utilidades sin dejar a un lado la seguridad (por ejemplo, permite utilizar el comando ping a diferencia de las jails ).","title":"Solaris Zones (2004)"},{"location":"#cgroups-y-namespaces-2006-2007","text":"En el 2006 se propuso un framework que agrupa procesos y aprovecha los mecanismos de control existentes del kernel . El objetivo es que los usuarios se centren en el controlador de recursos y se abstraigan de c\u00f3mo los procesos son gestionados y monitorizados. Este mecanismo en un principio se llamaba process containers pero se cambi\u00f3 el nombre a cgroups para diferenciarlo de los contenedores que conocemos hoy en d\u00eda, que usan otros mecanismos a parte de los cgroups . Por otro lado, en el 2002 se cre\u00f3 el primer namespace , el mount namespace . Aunque fue m\u00e1s adelante a partir del a\u00f1o 2007 cuando se comenz\u00f3 a ver el potencial que pod\u00edan tener junto a otras funcionalidades como los cgroups , as\u00ed que se empezaron a desarrollar nuevos namespaces . En la actualidad existen ocho.","title":"Cgroups y Namespaces (2006-2007)"},{"location":"#lxc-2008","text":"Los LXC se podr\u00edan considerar los primeros contenedores tal y como conocemos el concepto hoy en d\u00eda: fue la primera tecnolog\u00eda en aplicar los dos elementos principales de los contenedores: cgroups y namespaces . Permite al usuario comunicarse con las facilidades que ofrecen las funcionalidades del kernel mediante una interfaz en l\u00ednea de comandos . Sin embargo, no es una herramienta apropiada para realizar tareas de gesti\u00f3n de contenedores en un alto nivel.","title":"LXC (2008)"},{"location":"#lmctfy-y-docker-2013","text":"LMCTFY ( Let Me Containerize That For You ) fue la versi\u00f3n open-source del stack de Google a los contenedores de Linux. Su desarrollo finaliz\u00f3 en 2015 y Google comenz\u00f3 a transferir parte de la implementaci\u00f3n a libcontainer ahora mismo es una parte fundamental del stack de Docker y forma parte de OCI. Por otro lado, Docker supuso un despunte en la popularidad de los contenedores en el a\u00f1o de su salida y el crecimiento de ambos conceptos han ido de la mano desde entonces siendo hoy en d\u00eda l\u00edder en este \u00e1mbito . En sus inicios utilizaba LXC, pero m\u00e1s tarde lo sustituy\u00f3 por su propia librer\u00eda libcontainer .","title":"LMCTFY y Docker (2013)"},{"location":"#oci-2015","text":"OCI ( Open Container Initiative ) es un proyecto de la Linux Foundation cuyo objetivo es dise\u00f1ar un est\u00e1ndar abierto para la virtualizaci\u00f3n basada en contenedores . Fue establecida en 2015 por Docker y otros l\u00edderes de la industria. Despu\u00e9s del lanzamiento de Docker, surgi\u00f3 una comunidad alrededor de los contenedores. Sin embargo, con el paso del tiempo fueron apareciendo nuevas tecnolog\u00edas y herramientas que satisfac\u00edan las neceseidades que iban surgiendo. Este fue el motivo principal por el que surgi\u00f3 este est\u00e1ndar. Actualmente, OCI define dos especificaciones, aunque hablaremos m\u00e1s tarde de ellas en este documento .","title":"OCI (2015)"},{"location":"#otras-tecnologias","text":"A lo largo de las dos \u00faltimas d\u00e9cadas han ido surgiendo otras tecnolog\u00edas de virtualizaci\u00f3n de sistema operativo, pero han sido menos importantes para el ecosistema de los contenedores que las anteriormente mencionadas. Entre ellas podemos encontrar a Linux VServer (2001), OpenVZ (2005), Warden (2011), Singularity (2015) o Podman (2018).","title":"Otras tecnolog\u00edas"},{"location":"#arquitectura-de-los-contenedores","text":"Dada la ambig\u00fcedad con la que muchos profesionales se refieren a cada una de las capas que forman esta arquitectura y, dado que, dependiendo de la tecnolog\u00eda a usar, pueden cambiar ligeramente las funciones que realizan sus componentes, se utilizar\u00e1 como referencia el stack de Docker. Generalizaci\u00f3n de la arquitectura de los contenedores a partir de la arquitectura de Docker Comenzaremos el estudio por la parte inferior de la im\u00e1gen, los componentes del Linux Kernel, subiendo hasta llegar al Container Engine .","title":"Arquitectura de los contenedores"},{"location":"#1o-componentes-del-linux-kernel","text":"Existen numerosos componentes y herramientas del kernel que utilizan los contenedores para aislar los procesos. En concreto aqu\u00ed estudiaremos cinco de ellos, siendo los m\u00e1s importantes los dos primeros:","title":"1\u00ba Componentes del Linux Kernel"},{"location":"#namespaces","text":"Definici\u00f3n de namespaces Proporcionan el aislamiento entre procesos mediante la encapsulaci\u00f3n de ciertos recursos del sistema. De esta forma, hacen creer a los procesos dentro de un contenedor que tienen su propia instancia del recurso, aunque realmente lo est\u00e1n compartiendo. Esta herramienta impide que, mediante una vulnerabilidad en un contenedor, unos intrusos puedan acceder a la m\u00e1quina completa comprometi\u00e9ndola. La API del kernel que facilita esta caracter\u00edstica tiene tres llamadas principales: Llamada Descripci\u00f3n clone Crea un proceso hijo al igual que fork pero proporcionando m\u00e1s control sobre qu\u00e9 partes del contexto se comparten. unshare Crea un nuevo namespace y ejecuta un nuevo proceso dentro de \u00e9ste. setns Mueve el hilo actual dentro de un namespace existente ioctl Ofrece informaci\u00f3n sobre namespaces (la informaci\u00f3n se maneja desde /proc/$PID/ns ) En la actualidad existen 8 namespaces distintos, cada uno de ellos proporciona un distinto tipo de aislamiento : Tipos de namespaces MNT NS Mount Namespace Fue el primer namespace implementado. En aquel momento no se sab\u00eda que iban a crearse m\u00e1s tipos de namespaces as\u00ed que su flag correspondiente en los comandos de clone y unshare es CLONE_NEWNS . Funci\u00f3n Proporcionan aislamiento a las estructuras de datos que utiliza el sistema para gestionar los puntos de montaje . De esta forma, los procesos en distinto mount namespace tienen una visi\u00f3n distinta de la jerarqu\u00eda de los sistemas de archivos. Algunos usos que ofrece son: Cada usuario puede tener su propio /tmp para aumentar la seguridad frente a un usuario malicioso. Distintos procesos pueden tener un sistema de archivos ra\u00edz (es un concepto parecido a chroot ). Los puntos de montaje pueden ser privados o compartidos. UTS NS UNIX Time-Sharing Desde la implementaci\u00f3n del anterior namespace hasta este pasaron 4 a\u00f1os. Este es el m\u00e1s sencillo. Funci\u00f3n A\u00edsla el hostname y el domain-name del contenedor. La flag correspondiente es CLONE_NEWUTS . IPC NS Inter Process Communication Funci\u00f3n A\u00edsla cierta la compartici\u00f3n de ciertos recursos que facilitan la comunicaci\u00f3n como la memoria, objetos System V IPC y colas de mensajes POSIX. Sin embargo, despu\u00e9s de un clone se siguen compartiendo se\u00f1ales, sockets, descriptores de archivos o sondeos de memoria (habr\u00eda que aislarlos con otros namespaces ). Su flag correspondiente es CLONE_NETIPC . PID NS Process ID Los procesos de un sistema pertenecen a un \u00e1rbol de procesos global visible \u00fanicamente por el host . Funci\u00f3n Este namespace crea un nuevo \u00e1rbol de procesos propio de cada contenedor. Los procesos que est\u00e9n dentro de este namespace pertenecen , por tanto, a dos \u00e1rboles , el global y el propio del namespace . De esta forma un mismo proceso puede tener varios PIDs dependiendo de en qu\u00e9 \u00e1rbol se consulte. Distintos \u00e1rboles para distintos namespaces . El primer PID dentro del nuevo namespace siempre es el 1. Este proceso deber\u00eda tener unas caracter\u00edsticas \u00fanicas para poder funcionar como init y as\u00ed poder tener m\u00e1s de un proceso en un contenedor. Uno de los principales beneficios que presenta es que los contenedores se pueden migrar de una m\u00e1quina a otra manteniendo el \u00e1rbol de procesos. Su flag es CLONE_NEWPID . NET NS Network Funci\u00f3n Proporciona aislamiento de los recursos de red , donde cada contenedor puede tener su propio stack de recursos: tablas de enrutado, reglas de iptables , sockets , etc. En las tecnolog\u00edas actuales de contenedores se suele utilizar el modelo bridge . En este modelo el puente, que est\u00e1 en el namespace global, ofrece la conexi\u00f3n a los contenedores mediante una interfaz virtual. A la vez, cada contenedor, en su propio namespace , tiene una interfaz virtual conectada a la del puente. Ejemplo generalizado de bridge. La flag correspondiente es CLONE_NEWNET . User NS Funci\u00f3n Permite que los procesos puedan creer que est\u00e1n operando como root dentro de un contenedor, pero fuera del contenedor tienen realmente los privilegios de un usuario com\u00fan. Antes de que existiera este namespace si un proceso ten\u00eda permisos de root en cualquier entorno de aislamiento, tambi\u00e9n lo ten\u00eda en el sistema global, lo que era un problema muy grande en lo que a la seguridad se refiere. En este namespace el UID dentro del namespace siempre se corresponder\u00e1 con otro fuera de \u00e9l en el host. Este mapeo se puede hacer manualmente y elegir qu\u00e9 UID se selecciona dentro del contenedor para un usuario global. Un ejemplo de mapeo entre las dos tablas de usuarios se puede observar en la siguiente imagen. Ejemplo de mapeo entre un UID del host con un UID dentro del contenedor La flag correspondiente es CLONE_NETUSER . Cgroup NS Los cgroups (rupos de control en espa\u00f1ol) son una funcionalidad del kernel de Linux que trataremos en la secci\u00f3n que, b\u00e1sicamente, implementan monitorizaci\u00f3n y limitaci\u00f3n de los recursos que consume un grupo de procesos. Funci\u00f3n Virtualiza la vista de los grupos de control que ven los procesos. Sin esta restricci\u00f3n, un proceso podr\u00eda observar los grupos de control globales y acceder a la informaci\u00f3n sobre la limitaci\u00f3n de recursos de otros procesos. Tiene dos funciones principales: Evita que un grupo de control acceda a los l\u00edmites de otro superior a \u00e9l. Facilita la migraci\u00f3n de contenedores al aislar unos grupos de otros. De esta forma podemos ahorrar la necesidad de replicar los l\u00edmites seg\u00fan se van trasladando los contenedores. La flag correspondiente es CLONE_NEWCGROUP . Time NS Funci\u00f3n Virtualiza dos relojes del sistema permitiendo que, cuando un contenedor se migra de un nodo a otro, estos relojes son restaurados de forma consistente partiendo siempre del tiempo que ten\u00eda el reloj antes de ser migrado. Estos dos relojes son el CLOCK_MONOTONIC y CLOCK_BOOTTIME . La flag correspondiente es CLONE_NEWTIME .","title":"Namespaces"},{"location":"#control-groups","text":"Antes de que se crearan los grupos de control exist\u00edan formas para monitorizar y controlar procesos individuales , pero no hab\u00eda soporte para aplicar esas mismas operaciones a grupos de procesos . Por eso se acabaron implementando los Process Containers , que luego se renombraron a cgroups . Definici\u00f3n de Cgroups Son un framework basado en los mecanismos existentes del kernel para proporcionar una interfaz y un alcance m\u00e1s global a las operaciones de control y monitorizaci\u00f3n de procesos. Los grupos de control permiten repartir y asignar recursos entre un grupo de tareas o procesos del sistema. Los grupos de control ofrecen cuatro caracter\u00edsticas principales: Limitaci\u00f3n de recursos , como procesador, memoria, dispositivos E/S, etc. Priorizaci\u00f3n , que es parecida a la anterior caracter\u00edstica, pero no limita recursos a procesos sino que le da preferencia en el consumo de recursos a un proceso seleccionado. Monitorizaci\u00f3n de un grupo de procesos para obtener informaci\u00f3n de qu\u00e9 procesos est\u00e1n consumiendo cu\u00e1ntos recursos. Freezing de procesos . Esta es una herramienta que permite paralizar y retomar grupos de procesos. Es muy utilizada en el procesamiento por lotes. Existen dos versiones de Cgroups , hoy en d\u00eda se est\u00e1 intentando hacer la transici\u00f3n en la mayor\u00eda de herramientas a la segunda versi\u00f3n. Para comprender esta \u00faltima versi\u00f3n necesitamos entender tres conceptos: Cgroup : es un grupo de tareas al que se le asocia uno o m\u00e1s subsistemas. Subsistema o resource controller : representa un \u00fanico recurso, como la memoria o el tiempo de CPU. Jerarqu\u00eda : es el conjunto de todos los cgroups en forma de \u00e1rbol. Cada proceso del sistema est\u00e1 en un cgroup determinado. Cada nodo (o sea, cada cgroup ) del \u00e1rbol tiene asociados uno o m\u00e1s subsistemas. En la versi\u00f3n 2 de los cgroups existe una s\u00f3la jerarqu\u00eda. En la siguiente imagen podemos observar un ejemplo de jerarqu\u00eda, donde existen cuatro cgroups distintos. Cada uno de ellos tiene asociados varios subsistemas. En el caso del Group_2 los subsistemas asociados son \"memoria\" y \"pids\", y sus grupos hijos heredan los mismos subsistemas, pero pueden asociar otros nuevos o disociar los heredados. Ejemplo de jerarqu\u00eda de grupos de control. Cada grupo se a\u00f1ade creando un nuevo directorio dentro de la carpeta ra\u00edz /sys/fs/cgroup . En el momento de crear dicha carpeta, el sistema a\u00f1ade los archivos necesarios para gestionar los cgroups . \u00bfC\u00f3mo a\u00f1adimos los grupos de control de este ejemplo? Los subsistemas del siguiente ejemplo se a\u00f1adir\u00edan creando en primer lugar cada cgroup y m\u00e1s tarde escribiendo en los subsistemas en los archivos autogenerados: mkdir /sys/fs/cgroup/Cgroup_1 mkdir /sys/fs/cgroup/Cgroup_2 mkdir /sys/fs/cgroup/Cgroup_2.1 mkdir /sys/fs/cgroup/Cgroup_2.2 echo \"+memory +io\" > /sys/fs/cgroup/Group_1/cgroup.subtree_control echo \"+memory +pids\" > /sys/fs/cgroup/Group_2/cgroup.subtree_control echo \"+io\" > /sys/fs/cgroup/Group_2.1/cgroup.subtree_control echo \"-pids\" > /sys/fs/cgroup/Group_2.2/cgroup.subtree_control","title":"Control Groups"},{"location":"#union-filesystem","text":"Los sistemas de archivos por capas permiten compartir archivos en el disco, lo que supone un ahorro de espacio. Definici\u00f3n Los Union Filesystem son un tipo de sistema de archivos por capas. Permite a los archivos y directorios de sistemas de archivos distintos estar superpuestos formando un \u00fanico sistema de archivos .","title":"Union Filesystem"},{"location":"#overlayfs","text":"Docker, actualmente, utiliza un tipo de Union Filesystem llamado Overlay2. Esta tecnolog\u00eda utiliza tres capas: Base (s\u00f3lo lectura): es la capa donde van los archivos base. En terminolog\u00eda de Docker se corresponder\u00eda con la imagen. Overlay : es la capa donde opera el usuario. En un principio ofrece una vista de la capa base y permite operar sobre ella, aunque los cambios no se guardan en esta capa. Diff : es la capa donde se guardan los cambios realizados en la capa anterior. Una de las caracter\u00edsticas fundamentales que usan los UnionFS es la t\u00e9cnica de COW ( Copy On Write ) que permite reducir el consumo de copias sin modificar . Si un fichero existe en la capa base y otra capa quiere leerlo, lee el original. Cuando una capa quiera modificar el archivo es cuando debe copiarla y modificarla en la propia capa que la necesite. Las tres capas de Overlay2 junto a la t\u00e9cnica COW.","title":"OverlayFS"},{"location":"#capabilities","text":"El usuario con UID 0 es el root y tiene el control completo del sistema. Esto puede traer problemas recurrentes de seguridad (las Jails ya intentaron confinar el root omnipotente). Por eso, las capabilities proponen una soluci\u00f3n. Definici\u00f3n Las capabilities dividen los permisos que tiene un usuario en varias particiones (cada una de ellas es una capability ). Un buen ejemplo de su uso podr\u00eda ser la posibilidad de permitir a un binario poder crear un raw socket como en el caso de la instrucci\u00f3n ping mediante la capability CAP_NET_RAW sin necesidad de asignarle el resto de privilegios que ofrece el usuario root. Existen dos tipos de capabilities , unas son las asociadas a procesos y otros asociadas a archivos. Las que est\u00e1n asociadas a archivos se unen con las asociadas a procesos cuando un proceso quiere ejecutar un archivo. En la pr\u00e1ctica, los contenedores no necesitan todos los privilegios que ofrece el root. As\u00ed que, realmente, los usuarios root dentro de los contenedores tienen asignadas algunas capabilities para ofrecer \u00fanicamente ciertos permisos y restringir otros potencialmente inseguros .","title":"Capabilities"},{"location":"#pivot_root","text":"En la secci\u00f3n Historia de los contenedores ya explicamos la importancia que tuvo chroot para aparentar que cambiaba el directorio ra\u00edz de un proceso y de sus hijos. El problema que presenta es que no es segura, as\u00ed que es mejor utilizar la alternativa pivot_root . La funci\u00f3n completa de pivot_root es la siguiente: int pivot_root ( const char *new_root, const char *put_old ) ; Pivot_root mueve el punto de montaje ra\u00edz al directorio put_old haci\u00e9ndolo inaccesible para el proceso llamador como a todos sus hijos, mientras que convierte a new_root en el nuevo punto de montaje ra\u00edz . Esto soluciona las brechas de seguridad de chroot porque se aplica al mount namespace , as\u00ed que est\u00e1 cambiando \u00fanicamente en el namespace del contenedor la posibilidad de acceder a la antigua ra\u00edz.","title":"Pivot_root"},{"location":"#2o-container-runtimes","text":"\u00bfQu\u00e9 es un container runtime? Un container runtime es la herramienta o capa responsable de que el contenedor se ejecute correctamente (sin incluir los procesos que est\u00e1n dentro del contenedor). Este t\u00e9rmino puede tener diferentes significados dependiendo del proyecto o comunidad donde se consulte, principalmente debido a que el rango de tareas que realiza un runtime no est\u00e1 plenamente definido. En este documento vamos a separar los container runtimes seg\u00fan las tareas que realicen en low-level y high-level . Algunos ejemplos de low-level pueden ser runc, crun, gvisor o kata-runtime , mientras que entre los ejemplos de high-level podemos encontrarnos con containerd o CRI-O . \u00bfY Docker? \u00bfD\u00f3nde lo metemos? En muchas referencias se trata a Docker como un container runtime (refiri\u00e9ndose a Docker por su significado de daemon m\u00e1s su CLI). Llegados a este punto podemos entender perfectamente que se puede considerar de esta forma, ya que su funci\u00f3n es gestionar los contenedores y las im\u00e1genes (definici\u00f3n de high-level container runtime ). Sin embargo, en este documento se va a colocar en un escal\u00f3n superior, en los container engines . Esta decisi\u00f3n se ha tomado en base a dos razones: la primera es que una de las caracter\u00edsticas principales que hacen a esta herramienta m\u00e1s popular que containerd o CRI-O entre los usuarios es su capacidad de facilitar la comunicaci\u00f3n con el usuario. La segunda raz\u00f3n es que delega gran parte del trabajo en otro high-level runtime , que por defecto es containerd. Dicho esto, reiteramos que podr\u00eda ser perfectamente considerado un container runtime . De la misma forma, CRI-O o containerd se podr\u00edan llegar a considerar container engines si se utilizan a trav\u00e9s de plugins CLI que permitan al usuario interactuar debidamente. En resumen, la forma de referirse a estas tecnolog\u00edas no es universal y, como hemos dicho, dependiendo de la comunidad y el proyecto, las etiquetas pueden cambiar . Una forma de entender gr\u00e1ficamente el porqu\u00e9 de que estos t\u00e9rminos sean tan ambiguos se puede observar en la siguiente figura, que muestra de forma muy general y subjetiva el nivel al que operan algunos ejemplos de container runtimes . Nivel al que opera cada container runtime","title":"2\u00ba Container Runtimes"},{"location":"#low-level-container-runtimes","text":"En esta p\u00e1gina nos referiremos a los low-level runtimes a las capas que proporcionan las utilidades b\u00e1sicas como crear namespaces y comenzar el proceso de encapsular una aplicaci\u00f3n en un contenedor, es decir a las herramientas que se comunican directamente con el kernel .","title":"Low-level container runtimes"},{"location":"#runc-parte-del-stack-de-docker","text":"Definici\u00f3n Runc es un CLI que se encarga de crear contenedores y ejecutarlos seg\u00fan la especificaci\u00f3n que se le proporcione. Esta especificaci\u00f3n sigue el est\u00e1ndar OCI. La raz\u00f3n de que sea un low-level container runtime cuando realmente s\u00f3lo es un CLI es que antiguamente Docker utilizaba como container runtime LXC. Esto cambi\u00f3 m\u00e1s adelante y comenz\u00f3 a crear junto con Google una librer\u00eda propia que sustituyera a LXC: libcontainer . Hoy en d\u00eda, runc utiliza libcontainer , de hecho es su pieza b\u00e1sica y fundamental, por eso se pueden considerar que ambos son la misma capa. \u00bfQu\u00e9 est\u00e1ndar OCI? Los est\u00e1ndares OCI se introdujeron en la secci\u00f3n Historia de los contenedores . Consta de dos especificaciones: Imagen : define el formato del archivo de las im\u00e1genes OCI. El objetivo es permitir una serie de herramientas estandarizadas que puedan preparar, construir y transportar una imagen. Runtime : define la configuraci\u00f3n, entorno de ejecuci\u00f3n y ciclo de vida de un contenedor. Runc s\u00f3lo implementa la segunda especificaci\u00f3n ya que no entiende de im\u00e1genes, sino de bundles . Un bundle es un conjunto de archivos que contiene todos los datos necesarios para que un runtime pueda realizar todas las operaciones necesarias para crear y ejecutar un contenedor .","title":"runc (parte del stack de Docker)"},{"location":"#crun","text":"Se trata de otro low-level container runtime con funciones similares a runc. La diferencia principal radica en que \u00e9ste est\u00e1 escrito en C, mientras que runc est\u00e1 escrito en Go. Seg\u00fan el creador de crun, Go es un lenguaje multihilo por defecto, lo que no es recomendable para utilizar en un modelo fork/exec . Adem\u00e1s, utiliza los cgroups v2 , algo que no todas las herramientas de este \u00e1mbito soportan.","title":"crun"},{"location":"#otros-sandboxed-runtimes","text":"Las anteriores tecnolog\u00edas comparten sistema operativo con el host. Los Sandboxed runtimes son otro tipo de herramientas que aumentan el aislamiento con el host (aunque disminuyendo la eficiencia), lo que es recomendable en entornos donde varios clientes compartan las mismas m\u00e1quinas f\u00edsicas. Esto se logra separando los sistemas operativos del host y de los contenedores. Algunos ejemplos son gVisor (de Google), Firecracker (de Amazon) o Kata (de OpenStack). Comparaci\u00f3n gen\u00e9rica de Sandboxed Runtime y runc","title":"Otros Sandboxed Runtimes"},{"location":"#high-level-container-runtime","text":"En esta p\u00e1gina nos referimos a los high-level container runtimes a las capas que son responsables de la gesti\u00f3n y transporte de las im\u00e1genes de los contenedores, as\u00ed como de desempaquetarlas y enviarlas a los low-level container runtimes abstray\u00e9ndolos de esas actividades.","title":"High-level container runtime"},{"location":"#containerd-parte-del-stack-de-docker","text":"Es un est\u00e1ndar en la industria dada su portabilidad y facilidad de uso . Implementa ambas especificaciones OCI : Imagen : es capaz de desempaquetar im\u00e1genes OCI y de crear bundles a partir de ellas que el low-level puede entender. Runtime : puede comunicarse con un low-level que tambi\u00e9n implemente el est\u00e1ndar OCI y delegar el trabajo restante en \u00e9l. Tareas principales Hacer pull y push a las im\u00e1genes. Gestionar el almacenamiento de contenedores. Gestionar las redes y sus interfaces. Llamar al low-level container runtime con los par\u00e1metros deseados. Cabe destacar que containerd es un daemon , aunque dispone de varios plugins que aumentan sus funcionalidades en gran medida como una interfaz en l\u00ednea de comandos. Containerd-shim Si recordamos las capas que forman el stack de Docker, encontramos una pieza llamada containerd-shim que no se corresponde con ning\u00fan componente generalizado. Containerd-shim Containerd-shim es una implementaci\u00f3n que permite separar a los low-level container runtimes de containerd Cuando containerd quiere crear un contenedor, llama a runc creando una nueva instancia de runc por cada contenedor. Cuando runc termina la tarea de creaci\u00f3n se termina a si mismo. A partir de ahora es containerd-shim la pieza que se encarga de tomar las responsabilidades de ser padre del contenedor. Containerd-shim despu\u00e9s de que runc acabe su tarea","title":"Containerd (parte del stack de Docker)"},{"location":"#cri-o","text":"CRI (Container Runtime Interface) CRI ( Container Runtime Interface ) es la API que utiliza Kubernetes para hablar con los high-level container runtimes . El proyecto parti\u00f3 del hecho de que Kubernetes es un orquestrador que delega parte del trabajo en los runtimes . Si se establece una API (un est\u00e1ndar al fin y al cabo) que describa c\u00f3mo deben interactuar estas tecnolog\u00edas con Kubernetes se consigue aumentar en gran medida la interoperabilidad. CRI est\u00e1 basado en gRPC , que es un tipo de Remote Procedure Call desarrollado por Google que permite intercomunicaci\u00f3n entre varios lenguajes. Ahora ya podemos entender mejor qu\u00e9 es CRI-O y por qu\u00e9 se llama as\u00ed. Su desarrollo lo llev\u00f3 Red Hat con el objetivo de crear un runtime que soportara de forma nativa la comunicaci\u00f3n mediante CRI, sirviendo de puente entre Kubernetes y un low-level container runtime compatible con OCI. Comparaci\u00f3n con containerd En primer lugar, cabe destacar que existe un plugin de containerd que permite la comunicaci\u00f3n con Kubernetes mediante CRI. Un estudio encabezado por Lennart Espe en el que eval\u00faa el rendimiento de CRI-O y containerd utilizando como low-level tanto a runc como gVisor muestra que, en cuanto a rendimiento general , CRI-O m\u00e1s runc resulta ser la mejor opci\u00f3n. Cabe destacar que CRI-O est\u00e1 muy unido a Kubernetes y la gesti\u00f3n de las im\u00e1genes es recomendable encomend\u00e1rsela a otro container runtime o container engine como Podman (que es de Red Hat al igual que CRI-O).","title":"CRI-O"},{"location":"#3o-container-engines","text":"El container engine por excelencia es el propio Docker, formado por Docker daemon m\u00e1s sus APIs y CLI: Componentes Docker daemon Es el servicio que se ejecuta en la parte del host o servidor y escucha peticiones API. Es la herramienta que comienza la creaci\u00f3n del contenedor. API REST El deamon escicha las peticiones de los clientes a trav\u00e9s de esta API. CLI Es la forma con la que los usuarios, actuando como clientes, se comunican con el daemon a trav\u00e9s de la API.","title":"3\u00ba Container engines"},{"location":"#4o-otros-componentes","text":"Otros componentes que forman el entorno de los contenedores son las im\u00e1genes y los registros . Im\u00e1genes Una imagen es un archivo binario inmutable que contiene el c\u00f3digo, librer\u00edas, dependencias e informaci\u00f3n que necesita un runtime para crear un contenedor. El hecho de que sean archivos inmutables permite que equipos de desarrollo puedan probar software en un entorno estable y uniforme ya que representan un entorno en un momento y condiciones espec\u00edficas . En el caso particular de Docker existe un archivo llamado Dockerfile , que es un fichero de texto que contiene una lista de instrucciones que se van a ejecutar a la hora de crear la imagen, es decir, cuando se ejecute el comando docker build . El Dockerfile, habitualmente, utiliza como base otra imagen anterior y se le a\u00f1aden nuevas instrucciones, as\u00ed, una imagen se crea normalmente mediante el apilamiento de varias im\u00e1genes base. Registros Un registro es un sistema o repositorio que se encarga de almacenar y distribuir im\u00e1genes . Existen dos tipos de repositorios: P\u00fablicos : como Docker Hub. Ideal para equipos peque\u00f1os o usuarios que buscan una imagen que cumpla una funci\u00f3n espec\u00edfica sin importarles demasiado la privacidad y seguridad. Privados : s\u00f3lo se comparten con los usuarios deseados, normalmente, un equipo de trabajo. En la siguiente imagen podemos ver todos los componentes explicados interactuar entre s\u00ed. Todos los componentes explicados de Docker interactuando entre s\u00ed.","title":"4\u00ba Otros componentes"},{"location":"#4o-orquestadores","text":"Administrar un gran n\u00famero de contenedores es una tarea muy complicada, por eso surgieron ciertas tecnolog\u00edas que permiten automatizar el proceso de crear, desplegar y escalar contenedores . Estas tecnolog\u00edas son los orquestadores. Su principal objetivo es facilitar la gesti\u00f3n de contenedores localizados en muchos hosts. Esto, adem\u00e1s, facilita a los equipos de DevOps integrar sus flujos CI/CD. Tareas de los orquestadores Configurar y programar contenedores. Proporcionar el aprovisionamiento y despliegue adecuados de los contenedores. Asegurar la disponibilidad de los servicios. Escalar los contenedores. Controlar los recursos compartidos. Balancear la carga y el tr\u00e1fico. Monitorizar la salud de los contenedores. Proporcionar seguridad en las interacciones entre los contenedores.","title":"4\u00ba Orquestadores"},{"location":"#kubernetes","text":"Kubernetes es el est\u00e1ndar de facto en la actualidad en cuando a lo que orquestadores se refiere. Es una herramienta portable y de c\u00f3digo libre desarrollado originalmente por Google. Sus tres principios son seguridad, facilidad de uso y extensibilidad . Cuando se despliega una instancia de Kubernetes se est\u00e1 creando realmente un cluster . Este cluster tiene dos elementos fundamentales: control plane y compute machines o nodos : Componentes de un cluster de Kubernetes Control Plane API Server : maneja las peticiones internas y externas. El CLI kubelet que se suele usar se conecta a este componente. Scheduler : controla la salud de los pods y puede a\u00f1adir o quitar alguno. Controller Manager : controla que el cluster funciona correctamente (realmente est\u00e1 formado por varios controladores, cada uno con una funci\u00f3n espec\u00edfica). etcd : es una base de datos clave-valor distribuida. Permite almacenar la configuraci\u00f3n y el estado de los clusters . Nodos Pods : representa una instancia de una aplicaci\u00f3n, se corresponde con un contenedor o varios altamente acoplados. Container runtime/engine : se pueden utilizar varios como containerd, CRI-O o Docker. Kubelet : es la aplicaci\u00f3n que controla a los contenedores de un mismo pod . Proxy : facilita los servicios de red. Docker-shim Hab\u00edamos dicho que Kubernetes utiliza un CRI para comunicarse con los container engines y container runtimes . Como Docker no implementa CRI, la comunidad de Kubernetes cre\u00f3 una herramienta que permite comunicarse con Docker a trav\u00e9s de CRI. Sin embargo, en el a\u00f1o 2020 la comunidad de Kubernetes anunci\u00f3n que no va a seguir manteniendo este plugin, as\u00ed que el uso de Kubernetes m\u00e1s Docker tiene un futuro incierto. Docker-shim haciendo de puente entre Kubernetes y Docker.","title":"Kubernetes"},{"location":"#kubernetes-y-sus-container-runtimes","text":"Como hemos visto hasta ahora existen muchas opciones a la hora de elegir container runtime para Kubernetes. Cada opci\u00f3n tiene sus pros y sus contras en la pr\u00e1ctica, sin embargo, si analizamos las capas que estar\u00edamos a\u00f1adiendo en cada opci\u00f3n hay ciertas diferencias que merece la pena destacar. Todos los componentes explicados de Docker interactuando entre s\u00ed. Como se puede comprobar en la \u00faltima imagen, dependiendo de la opci\u00f3n que escojamos, vamos a encontrarnos m\u00e1s componentes o menos en el stack. Esto no quiere decir que los stacks con menos saltos sean m\u00e1s eficientes: cada alternativa tiene sus pros y sus contras que habr\u00eda que estudiar con mayor detenimiento.","title":"Kubernetes y sus Container runtimes"},{"location":"#ampliacion-de-contenidos","text":"Los contenidos explicados en esta pagina han sido obtenidos \u00edntegramente de * ENLACE A LA MEMORIA EN GITHUB *","title":"AMPLIACI\u00d3N DE CONTENIDOS"},{"location":"tutoriales/contenedorbash/","text":"Contenedor en Bash CUIDADO Es muy recomendable utilizar una MV para realizar los siguientes pasos ya que pueden provocar problemas en el sistema si no se realizan correctamente. Namespaces Para crear un contenedor en terminal necesitamos muchas menos instrucciones que para crearlo en Go: contamos con comandos que facilitan enormemente la tarea. En el siguiente ejemplo se crear\u00e1 lo mismo que en el tutorial de Go pero en tan s\u00f3lo unas pocas l\u00edneas. Para el caso de los namespaces simplemente necesitamos utilizar la orden unshare : en este caso crearemos los correspondientes a --mount , --pid , --uts y --user . Adem\u00e1s, vamos a especificar con --fork que la terminal que va a abrir el programa sea un hijo del proceso unshare (es \u00fatil precisamente porque hemos creado un nuevo Namespace PID ). Con --map-root-user ahorramos todas las l\u00edneas que escrib\u00edamos en Go para hacer que el usuario del nuevo Namespace USER sea root. Por \u00faltimo, podemos ahorrar realizar el pivot_root del que ya hemos hablado anteriormente con la opci\u00f3n root=[direcci\u00f3n] . Por otro lado, es importante acordarse de montar el nuevo pseudo-filesystem de /proc . root@bar~$ unshare --mount --pid --uts --user --fork --map-root-user --root = alpinefs /bin/sh root@bar~$ mount -t proc proc proc root@bar~$ hostname demo Cgroups En el caso de los grupos de control, tiene la misma dificultad que en el ejemplo en Go: simplemente hay que crear los directorios correspondientes para cada tipo de Cgroup , eso s\u00ed, desde una terminal con root en el Host, ya que tenemos que introducir el PID en los correspondientes cgroup.procs del Namespace USER superior (es decir, el que vemos desde el host). root @ bar ~$ ps aux | grep / sh root 2552 pts / 0 00 : 13 / bin / sh root @ bar ~$ mkdir / sys / fs / cgroup / pids / demo root @ bar ~$ mkdir / sys / fs / cgroup / memory / demo root @ bar ~$ echo 20 / sys / fs / cgroup / pids / demo / pids . max root @ bar ~$ echo 2552 / sys / fs / cgroup / pids / demo / cgroup . procs root @ bar ~$ echo \"2M\" / sys / fs / cgroup / memory / demo / memory . limit_in_bytes root @ bar ~$ echo 2552 / sys / fs / cgroup / memory / demo / cgroup . procs","title":"Tutorial Bash"},{"location":"tutoriales/contenedorbash/#contenedor-en-bash","text":"CUIDADO Es muy recomendable utilizar una MV para realizar los siguientes pasos ya que pueden provocar problemas en el sistema si no se realizan correctamente.","title":"Contenedor en Bash"},{"location":"tutoriales/contenedorbash/#namespaces","text":"Para crear un contenedor en terminal necesitamos muchas menos instrucciones que para crearlo en Go: contamos con comandos que facilitan enormemente la tarea. En el siguiente ejemplo se crear\u00e1 lo mismo que en el tutorial de Go pero en tan s\u00f3lo unas pocas l\u00edneas. Para el caso de los namespaces simplemente necesitamos utilizar la orden unshare : en este caso crearemos los correspondientes a --mount , --pid , --uts y --user . Adem\u00e1s, vamos a especificar con --fork que la terminal que va a abrir el programa sea un hijo del proceso unshare (es \u00fatil precisamente porque hemos creado un nuevo Namespace PID ). Con --map-root-user ahorramos todas las l\u00edneas que escrib\u00edamos en Go para hacer que el usuario del nuevo Namespace USER sea root. Por \u00faltimo, podemos ahorrar realizar el pivot_root del que ya hemos hablado anteriormente con la opci\u00f3n root=[direcci\u00f3n] . Por otro lado, es importante acordarse de montar el nuevo pseudo-filesystem de /proc . root@bar~$ unshare --mount --pid --uts --user --fork --map-root-user --root = alpinefs /bin/sh root@bar~$ mount -t proc proc proc root@bar~$ hostname demo","title":"Namespaces"},{"location":"tutoriales/contenedorbash/#cgroups","text":"En el caso de los grupos de control, tiene la misma dificultad que en el ejemplo en Go: simplemente hay que crear los directorios correspondientes para cada tipo de Cgroup , eso s\u00ed, desde una terminal con root en el Host, ya que tenemos que introducir el PID en los correspondientes cgroup.procs del Namespace USER superior (es decir, el que vemos desde el host). root @ bar ~$ ps aux | grep / sh root 2552 pts / 0 00 : 13 / bin / sh root @ bar ~$ mkdir / sys / fs / cgroup / pids / demo root @ bar ~$ mkdir / sys / fs / cgroup / memory / demo root @ bar ~$ echo 20 / sys / fs / cgroup / pids / demo / pids . max root @ bar ~$ echo 2552 / sys / fs / cgroup / pids / demo / cgroup . procs root @ bar ~$ echo \"2M\" / sys / fs / cgroup / memory / demo / memory . limit_in_bytes root @ bar ~$ echo 2552 / sys / fs / cgroup / memory / demo / cgroup . procs","title":"Cgroups"},{"location":"tutoriales/contenedorgo/","text":"Tutorial ontenedor en Go CUIDADO Es muy recomendable utilizar una MV para realizar los siguientes pasos ya que pueden provocar problemas en el sistema si no se realizan correctamente. Nuestro objetivo es que, al acabar este tutorial, tengamos un programa que sea capaz de crear un proceso y aislarlo con namespaces y cgroups . De hecho, intentaremos que la interacci\u00f3n con \u00e9ste sea muy parecida a la que tendr\u00edamos cuando ejecutamos un contenedor Docker: # Nosotros vamos a ejecutarlo as\u00ed: root@bar:~$ go run contenedor.go run <command> <args> # Una ejecuci\u00f3n en Docker ser\u00eda algo de este estilo: root@bar:~$ docker run <image> <command> <args> Cabe destacar que es necesario que todos los ficheros est\u00e9n en una carpeta cuyo grupo y usuario pertenezca a root , as\u00ed como realizar todos los comandos con privilegios de root. Paso 1: Creaci\u00f3n del c\u00f3digo base El primer paso consiste en escribir las primeras dos funciones : main , que simplemente comprobar\u00e1 que se ha ejecutado el comando correcto en terminal y run , que imprimir\u00e1 en pantalla los datos del proceso y ejecutar\u00e1 otro nuevo que le indiquemos en par\u00e1metro. package main import ( \"fmt\" \"os\" \"os/exec\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . Run () } Ahora puedes probar a ejecutar la instrucci\u00f3n go run contenedor.go run ps a y comprobar que se existen dos procesos: el contenedor.go y el ps a que le hemos indicado que ejecute dentro del \"pre-contenedor\". Explicaci\u00f3n La funci\u00f3n run simplemente imprime por pantalla informaci\u00f3n \u00fatil sobre el proceso que estamos ejecutando y que, m\u00e1s adelante, crear\u00e1 el contenedor. De momento, lo \u00fanico que estamos haciendo es indicarle que queremos ejecutar un comando con la funci\u00f3n Command del paquete exec indic\u00e1ndole los argumentos. Este comando devuelve una estructura del tipo Cmd en la que tenemos que especificarle el Stdin Stdout y Stderr . Tambi\u00e9n podemos ejecutar otros comandos dentro del contenedor, como go run contenedor.go run /bin/bash , en cuyo caso se abrir\u00e1 una nueva terminal. Paso 2: Aislando con Namespace UTS (Hostname) Este namespace permite cambiar tanto el hostname como el domain-name del contenedor sin que afecte a estos campos del host. Para lograr este aislamiento debemos a\u00f1adir las siguientes l\u00edneas , adem\u00e1s de importar el paquete necesario syscall : cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS , } Con estos cambios lograremos que, al iniciar el contenedor con go run contenedor.go run /bin/bash , podamos cambiar el hostname dentro del contenedor y que, al salir del mismo (saliendo del bash con un exit ) no haya cambiado en el host. C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS , } cmd . Run () } Paso 3: Aislando con Namespace USER (username) Con la inclusi\u00f3n de este namespace vamos a separar las tablas de UID y GID entre el host y el contenedor , de tal forma que dentro del contenedor no haya los mismos usuarios que fuera. Para crear el nuevo namespace simplemente es necesario a\u00f1adir una flag m\u00e1s al c\u00f3digo: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , } El problema es que ahora mismo, cuando ejecutamos el contenedor, nos informa que el proceso que lo llama tiene UID 0 pero si comprobamos el usuario que se nos asign\u00f3 en la nueva tabla de UID (o sea, en la tabla del contenedor) es un usuario \"aleatorio\": root@bar:~$ go run contenedor.go run /bin/bash Corriendo '[/bin/bash]' con User ID 0 en PID 2792 root@bar:~$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) As\u00ed que le vamos a indicar que mapee el usuario de fuera del contenedor (UID 0) con el que queramos dentro : cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del contenedor HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo un usuario }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del container HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo unuser }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getpid (), Size : 1 , }, }, } cmd . Run () } Paso 4: Aislando con Namespace NS (Mount) Este fue el primer Namespace que se incluy\u00f3 en el kernel de Linux y uno de los m\u00e1s sencillos: simplemente aisla los puntos de montaje. De esta forma podemos esconder los mounts entre el host y el contenedor y viceversa . Para ver los puntos de montaje usados en cada una de las m\u00e1quinas con el comando mount . Para a\u00f1adir esta caracter\u00edstica debemos incluir la flag apropiada: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWNS | syscall . CLONE_NEWUSER , { ... } } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_ syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del container HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo unuser }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getpid (), Size : 1 , }, }, } cmd . Run () } Paso 5: Aislando con Namespace PID El PID namespace permite separar los \u00e1rboles de procesos, de tal forma que dentro del contenedor no se pueden ver los procesos del host . Para a\u00f1adir este namespace simplemente incluimos la flag apropiada: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , { ... } } Sin embargo, cuando ejecutamos un ps a seguimos pudiendo ver los mismos procesos de antes. Explicaci\u00f3n: Mount Namespace no a\u00edsla los procesos Es importante saber que /proc es un pseudo-filesystem montado por el sistema operativo por defecto donde se muestra la informaci\u00f3n sobre los procesos. Cuando hacemos un ps a , lo que est\u00e1 pasando realmente es que esta instrucci\u00f3n est\u00e1 consultando los datos del directorio anteriormente nombrado. La soluci\u00f3n es asignar un nuevo /proc en la ra\u00edz del contenedor. Para ello necesitamos un nuevo root filesystem como Alpine (que continene \u00fanicamente los archivos necesarios para que funcione un contenedor). Paso 6: A\u00f1adiendo un Filesystem para el contenedor Para realizar este paso necesitamos descargar el mini-root de Alpine. Lo descomprimimos y lo llamamos, por ejemplo, alpinefs y le cambiamos el usuario con chown root alpinefs/ . Montamos nuestro propio /proc Necesitamos un nuevo directorio proc para que el comando ps a pueda acceder a \u00e9l para acceder a la informaci\u00f3n de los procesos del contnedor. Explicaci\u00f3n: directorio /proc Otra cosa que se podr\u00eda intuir es que es necesario a\u00f1adir el Namespace NS (de Mount) para aislar ambos directorios. Pero no, este \u00faltimo comentario es falso pese a que existan muchas referencias en la red a que es completamente necesario: cuando un proceso como ps quiere comprobar los procesos activos en /proc lo que hace es ir directamente a ese archivo. Nuestro proceso, tanto con el Namespace NS como sin \u00e9l, va a seguir mirando los procesos en la carpeta /proc , es decir, la que est\u00e1 justo debajo del directorio ra\u00edz y no en la del nuevo root filesystem de alpine. As\u00ed que podr\u00edamos montar nuestro nuevo proc/ sin el Namespace NS . La soluci\u00f3n de que se muestren \u00fanicamente los procesos activos de nuestro contenedor se divide en dos pasos, pero antes, debemos cambiar un poco la forma en la que hab\u00edamos planteado el programa en un principio. Ahora, en vez de ejecutar desde la funci\u00f3n run la instrucci\u00f3n indicada en los par\u00e1metros, vamos a duplicar el proceso actual llamando a /proc/self/exe para que en esta segunda ejecuci\u00f3n se cambie el flujo del programa y no pase por la funci\u00f3n run , sino por la funci\u00f3n child . cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) De esta forma, habr\u00eda otra funci\u00f3n dentro del programa que se ejecutar\u00eda la segunda vez, donde implementamos la soluci\u00f3n a nuestro \u00faltimo problema: Hacer la nueva ra\u00edz de nuestro contenedor la ra\u00edz del filesystem que acabamos de descargar ( alpinefs/ ) para que al acceder a /proc est\u00e9 accediendo al nuevo y no al del Host. Esto se puede hacer tanto con la llamada al sistema chroot o pivot_root . La segunda opci\u00f3n es m\u00e1s segura, aunque m\u00e1s complicada. Por lo tanto, para evitar aumentar demasiado la complejidad se utilizar\u00e1 el primer m\u00e9todo (anexando el segundo al final del tutorial). Montar el filesystem proc para que el sistema pueda utilizarlo para almacenar informaci\u00f3n sobre los procesos. func child () { fmt . Printf ( \"Running '%v' as user %d in PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) must ( syscall . Chroot ( \"alpinefs/\" )) must ( os . Chdir ( \"/\" )) must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } Explicaci\u00f3n: \u00bfpor qu\u00e9 crear una nueva funci\u00f3n? Ahora no s\u00f3lo vamos a a\u00f1adir namespaces y ejecutar una instrucci\u00f3n sino que vamos a realizar otras acciones. Si cogemos el flujo de la funci\u00f3n run y realizamos las nuevas acciones despu\u00e9s de cmd.Run() no se estar\u00edan completando hasta que acabara esta \u00faltima orden. A su vez, si introducimos las acciones antes de cmd.Run() no se habr\u00edan creado a\u00fan los_namespaces: es justo mientras transcurre en cmd.Run() cuando queremos modificar el contenedor. Por eso una opci\u00f3n es obligar al proceso a llamarse a una copia de s\u00ed mismo y cambiar el flujo del programa a la nueva funci\u00f3n child . Cabe destacar que el filesystem propuesto de Alpine no cuenta con Bash, as\u00ed que tendr\u00edamos que mandar ejecutar /bin/sh Por otro lado, es recomendable que a partir de ahora empezemos a manejar los errores que nos puedan aparecer: func must ( err error ) { if err != nil { panic ( err ) } } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () case \"child\" : child () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getuid (), Size : 1 , }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } must ( cmd . Run ()) } func child () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) must ( syscall . Sethostname ([] byte ( \"container\" ))) must ( syscall . Chroot ( \"alpinefs/\" )) must ( os . Chdir ( \"/\" )) must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } func must ( err error ) { if err != nil { panic ( err ) } } \u00bfQU\u00c9 HEMOS CONSEGUIDO HASTA AHORA? En estos momentos hemos conseguido introducir unos cuantos namespaces , al menos los m\u00e1s significativos para realizar en este tutorial. El hostname namespace se puede comprobar de esta forma: # Fuera del contenedor root@bar:~$ hostname host root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ sethostname contenedor root@bar:~$ exit # Fuera del contenedor root@bar:~$ hostname host El user namespace lo hemos conseguido introducir a\u00f1adiendo los mapeos de usuario a root dentro del contenedor. Lo podemos comprobar de esta forma: # Fuera del contenedor root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ id uid=0(root) gid=0(root) groups=0(root) El mount namespace se puede comprobar de una forma muy sencilla: # Fuera del contenedor root@bar:~$ mount # ##### Aparecen muchos puntos de montaje usados por el host root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ mount proc on /proc type proc (rw,relatime) El pid namespace lo podemos comprobar realizando las siguientes instrucciones: # Fuera del contenedor root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ ps a PID USER TIME COMMAND 1 root 0:00 /proc/self/exe child /bin/sh 5 root 0:00 /bin/sh 11 root 0:00 ps a A\u00f1adiendo Cgroups (memoria y PID) En este ejemplo a\u00f1adiremos un l\u00edmite al n\u00famero m\u00e1ximo de procesos en el cgroup (y, por lo tanto, en el contenedor) permitidos. Para ello necesitamos crear un nuevo directorio en /sys/fs/cgroup/pids/ . Al crear el directorio autom\u00e1ticamente el sistema a\u00f1ade los archivos necesarios para mostrar los datos del nuevo Cgroup y para modificar los l\u00edmites que se le quieran a\u00f1adir. En nuestro caso el grupo se llamar\u00e1 demo . Para modificar el n\u00famero m\u00e1ximo de procesos que se permite en el contenedor s\u00f3lo es necesario modificar el archivo donde se indica el n\u00famero (pondremos como m\u00e1ximo 12 procesos) y otro donde se introduce al proceso del contenedor en el grupo de control. Adem\u00e1s a\u00f1adiremos un n\u00famero m\u00e1ximo de bytes de memoria que se le asignan al contenedor, aunque esto es m\u00e1s complicado de comprobar que funciona correctamente, pero los pasos son los mismos que en el anterior caso. func cg () cgroups := \"/sys/fs/cgroup\" // Creando cgroup para PIDs pids := filepath . Join ( cgroups , \"pids/demo\" ) if _ , err := os . Stat ( pids ); os . IsNotExist ( err ) { must ( os . Mkdir ( pids , 0755 )) } // Creando cgroup para PIDs memory := filepath . Join ( cgroups , \"memory/demo\" ) if _ , err := os . Stat ( memory ); os . IsnotExist ( err ) { must ( os . Mkdir ( memory , 0755 )) } //Establecemos limite y metemos al proceso dentro del grupo de procesos must ( ioutil . WriteFile ( filepath . Join ( pids , \"pids.max\" ), [] byte ( \"10\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( pids , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"memory.limit_in_bytes\" ), [] byte ( \"2M\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) } S\u00f3lo hace falta llamar a esta funci\u00f3n desde el principio de la funci\u00f3n child . Anexo: mejora con pivot_root Cuando se introdujo en el tutorial la llamada al sistema chroot se mencion\u00f3 la posibilidad de utilizar otra m\u00e1s segura: pivot_root . Aunque antiguamente, en los primeros contenedores, se utilizaba la primera opci\u00f3n, se ha llegado a la conclusi\u00f3n de que tiene varios fallos de seguridad que permiten \"salir o escapar\" del contenedor. pivot_root aprovecha el mount namespace ya que permite hacer unmount del antiguo root y no lo hace accesible en el namespace del contenedor. Si usamos \u00fanicamente chroot podemos acceder al Host con el siguiente comando: chroot /proc/1/root . Lo que hay que saber para poder usar pivot_root es que necesita dos argumentos, el primero es la direcci\u00f3n del nuevo directorio ra\u00edz (no viene en la documentaci\u00f3n pero debe estar montado sobre s\u00ed mismo con la opci\u00f3n bind ) y el segundo es la direcci\u00f3n donde se va a situar el antiguo directorio ra\u00edz. El c\u00f3digo completo del tutorial quedar\u00eda as\u00ed: C\u00d3DIGO COMPLETO package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" \"io/ioutil\" \"strconv\" \"path/filepath\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () case \"child\" : child () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getuid (), Size : 1 , }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } must ( cmd . Run ()) } func child () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cg () must ( syscall . Sethostname ([] byte ( \"container\" ))) pivot () must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr must ( syscall . Unmount ( \".old_root\" , syscall . MNT_DETACH )) must ( os . Remove ( \".old_root\" )) defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } func cg () { cgroups := \"/sys/fs/cgroup\" pids := filepath . Join ( cgroups , \"pids/demo\" ) if _ , err := os . Stat ( pids ); os . IsNotExist ( err ) { must ( os . Mkdir ( pids , 0755 )) } memory := filepath . Join ( cgroups , \"memory/demo\" ) if _ , err := os . Stat ( memory ); os . IsNotExist ( err ) { must ( os . Mkdir ( memory , 0755 )) } must ( ioutil . WriteFile ( filepath . Join ( pids , \"pids.max\" ), [] byte ( \"22\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( pids , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"memory.limit_in_bytes\" ), [] byte ( \"2M\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) } func pivot () { must ( syscall . Mount ( \"alpinefs\" , \"alpinefs\" , \"\" , syscall . MS_BIND | syscall . MS_REC , \"\" )) if _ , err := os . Stat ( \"alpinefs/.old_root\" ); os . IsNotExist ( err ) { must ( os . Mkdir ( \"alpinefs/.old_root\" , 0700 )) } must ( syscall . PivotRoot ( \"alpinefs\" , \"alpinefs/.old_root\" )) must ( os . Chdir ( \"/\" )) } func must ( err error ) { if err != nil { panic ( err ) } }","title":"Tutorial Go"},{"location":"tutoriales/contenedorgo/#tutorial-ontenedor-en-go","text":"CUIDADO Es muy recomendable utilizar una MV para realizar los siguientes pasos ya que pueden provocar problemas en el sistema si no se realizan correctamente. Nuestro objetivo es que, al acabar este tutorial, tengamos un programa que sea capaz de crear un proceso y aislarlo con namespaces y cgroups . De hecho, intentaremos que la interacci\u00f3n con \u00e9ste sea muy parecida a la que tendr\u00edamos cuando ejecutamos un contenedor Docker: # Nosotros vamos a ejecutarlo as\u00ed: root@bar:~$ go run contenedor.go run <command> <args> # Una ejecuci\u00f3n en Docker ser\u00eda algo de este estilo: root@bar:~$ docker run <image> <command> <args> Cabe destacar que es necesario que todos los ficheros est\u00e9n en una carpeta cuyo grupo y usuario pertenezca a root , as\u00ed como realizar todos los comandos con privilegios de root.","title":"Tutorial ontenedor en Go"},{"location":"tutoriales/contenedorgo/#paso-1-creacion-del-codigo-base","text":"El primer paso consiste en escribir las primeras dos funciones : main , que simplemente comprobar\u00e1 que se ha ejecutado el comando correcto en terminal y run , que imprimir\u00e1 en pantalla los datos del proceso y ejecutar\u00e1 otro nuevo que le indiquemos en par\u00e1metro. package main import ( \"fmt\" \"os\" \"os/exec\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . Run () } Ahora puedes probar a ejecutar la instrucci\u00f3n go run contenedor.go run ps a y comprobar que se existen dos procesos: el contenedor.go y el ps a que le hemos indicado que ejecute dentro del \"pre-contenedor\". Explicaci\u00f3n La funci\u00f3n run simplemente imprime por pantalla informaci\u00f3n \u00fatil sobre el proceso que estamos ejecutando y que, m\u00e1s adelante, crear\u00e1 el contenedor. De momento, lo \u00fanico que estamos haciendo es indicarle que queremos ejecutar un comando con la funci\u00f3n Command del paquete exec indic\u00e1ndole los argumentos. Este comando devuelve una estructura del tipo Cmd en la que tenemos que especificarle el Stdin Stdout y Stderr . Tambi\u00e9n podemos ejecutar otros comandos dentro del contenedor, como go run contenedor.go run /bin/bash , en cuyo caso se abrir\u00e1 una nueva terminal.","title":"Paso 1: Creaci\u00f3n del c\u00f3digo base"},{"location":"tutoriales/contenedorgo/#paso-2-aislando-con-namespace-uts-hostname","text":"Este namespace permite cambiar tanto el hostname como el domain-name del contenedor sin que afecte a estos campos del host. Para lograr este aislamiento debemos a\u00f1adir las siguientes l\u00edneas , adem\u00e1s de importar el paquete necesario syscall : cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS , } Con estos cambios lograremos que, al iniciar el contenedor con go run contenedor.go run /bin/bash , podamos cambiar el hostname dentro del contenedor y que, al salir del mismo (saliendo del bash con un exit ) no haya cambiado en el host. C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS , } cmd . Run () }","title":"Paso 2: Aislando con Namespace UTS (Hostname)"},{"location":"tutoriales/contenedorgo/#paso-3-aislando-con-namespace-user-username","text":"Con la inclusi\u00f3n de este namespace vamos a separar las tablas de UID y GID entre el host y el contenedor , de tal forma que dentro del contenedor no haya los mismos usuarios que fuera. Para crear el nuevo namespace simplemente es necesario a\u00f1adir una flag m\u00e1s al c\u00f3digo: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , } El problema es que ahora mismo, cuando ejecutamos el contenedor, nos informa que el proceso que lo llama tiene UID 0 pero si comprobamos el usuario que se nos asign\u00f3 en la nueva tabla de UID (o sea, en la tabla del contenedor) es un usuario \"aleatorio\": root@bar:~$ go run contenedor.go run /bin/bash Corriendo '[/bin/bash]' con User ID 0 en PID 2792 root@bar:~$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) As\u00ed que le vamos a indicar que mapee el usuario de fuera del contenedor (UID 0) con el que queramos dentro : cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del contenedor HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo un usuario }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del container HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo unuser }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getpid (), Size : 1 , }, }, } cmd . Run () }","title":"Paso 3: Aislando con Namespace USER (username)"},{"location":"tutoriales/contenedorgo/#paso-4-aislando-con-namespace-ns-mount","text":"Este fue el primer Namespace que se incluy\u00f3 en el kernel de Linux y uno de los m\u00e1s sencillos: simplemente aisla los puntos de montaje. De esta forma podemos esconder los mounts entre el host y el contenedor y viceversa . Para ver los puntos de montaje usados en cada una de las m\u00e1quinas con el comando mount . Para a\u00f1adir esta caracter\u00edstica debemos incluir la flag apropiada: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWNS | syscall . CLONE_NEWUSER , { ... } } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_ syscall . CLONE_NEWUSER , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , // UID dentro del container HostID : os . Getuid (), // UID en el host Size : 1 , // Quiero mapear solo unuser }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getpid (), Size : 1 , }, }, } cmd . Run () }","title":"Paso 4: Aislando con Namespace NS (Mount)"},{"location":"tutoriales/contenedorgo/#paso-5-aislando-con-namespace-pid","text":"El PID namespace permite separar los \u00e1rboles de procesos, de tal forma que dentro del contenedor no se pueden ver los procesos del host . Para a\u00f1adir este namespace simplemente incluimos la flag apropiada: cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , { ... } } Sin embargo, cuando ejecutamos un ps a seguimos pudiendo ver los mismos procesos de antes. Explicaci\u00f3n: Mount Namespace no a\u00edsla los procesos Es importante saber que /proc es un pseudo-filesystem montado por el sistema operativo por defecto donde se muestra la informaci\u00f3n sobre los procesos. Cuando hacemos un ps a , lo que est\u00e1 pasando realmente es que esta instrucci\u00f3n est\u00e1 consultando los datos del directorio anteriormente nombrado. La soluci\u00f3n es asignar un nuevo /proc en la ra\u00edz del contenedor. Para ello necesitamos un nuevo root filesystem como Alpine (que continene \u00fanicamente los archivos necesarios para que funcione un contenedor).","title":"Paso 5: Aislando con Namespace PID"},{"location":"tutoriales/contenedorgo/#paso-6-anadiendo-un-filesystem-para-el-contenedor","text":"Para realizar este paso necesitamos descargar el mini-root de Alpine. Lo descomprimimos y lo llamamos, por ejemplo, alpinefs y le cambiamos el usuario con chown root alpinefs/ .","title":"Paso 6: A\u00f1adiendo un Filesystem para el contenedor"},{"location":"tutoriales/contenedorgo/#montamos-nuestro-propio-proc","text":"Necesitamos un nuevo directorio proc para que el comando ps a pueda acceder a \u00e9l para acceder a la informaci\u00f3n de los procesos del contnedor. Explicaci\u00f3n: directorio /proc Otra cosa que se podr\u00eda intuir es que es necesario a\u00f1adir el Namespace NS (de Mount) para aislar ambos directorios. Pero no, este \u00faltimo comentario es falso pese a que existan muchas referencias en la red a que es completamente necesario: cuando un proceso como ps quiere comprobar los procesos activos en /proc lo que hace es ir directamente a ese archivo. Nuestro proceso, tanto con el Namespace NS como sin \u00e9l, va a seguir mirando los procesos en la carpeta /proc , es decir, la que est\u00e1 justo debajo del directorio ra\u00edz y no en la del nuevo root filesystem de alpine. As\u00ed que podr\u00edamos montar nuestro nuevo proc/ sin el Namespace NS . La soluci\u00f3n de que se muestren \u00fanicamente los procesos activos de nuestro contenedor se divide en dos pasos, pero antes, debemos cambiar un poco la forma en la que hab\u00edamos planteado el programa en un principio. Ahora, en vez de ejecutar desde la funci\u00f3n run la instrucci\u00f3n indicada en los par\u00e1metros, vamos a duplicar el proceso actual llamando a /proc/self/exe para que en esta segunda ejecuci\u00f3n se cambie el flujo del programa y no pase por la funci\u00f3n run , sino por la funci\u00f3n child . cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) De esta forma, habr\u00eda otra funci\u00f3n dentro del programa que se ejecutar\u00eda la segunda vez, donde implementamos la soluci\u00f3n a nuestro \u00faltimo problema: Hacer la nueva ra\u00edz de nuestro contenedor la ra\u00edz del filesystem que acabamos de descargar ( alpinefs/ ) para que al acceder a /proc est\u00e9 accediendo al nuevo y no al del Host. Esto se puede hacer tanto con la llamada al sistema chroot o pivot_root . La segunda opci\u00f3n es m\u00e1s segura, aunque m\u00e1s complicada. Por lo tanto, para evitar aumentar demasiado la complejidad se utilizar\u00e1 el primer m\u00e9todo (anexando el segundo al final del tutorial). Montar el filesystem proc para que el sistema pueda utilizarlo para almacenar informaci\u00f3n sobre los procesos. func child () { fmt . Printf ( \"Running '%v' as user %d in PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) must ( syscall . Chroot ( \"alpinefs/\" )) must ( os . Chdir ( \"/\" )) must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } Explicaci\u00f3n: \u00bfpor qu\u00e9 crear una nueva funci\u00f3n? Ahora no s\u00f3lo vamos a a\u00f1adir namespaces y ejecutar una instrucci\u00f3n sino que vamos a realizar otras acciones. Si cogemos el flujo de la funci\u00f3n run y realizamos las nuevas acciones despu\u00e9s de cmd.Run() no se estar\u00edan completando hasta que acabara esta \u00faltima orden. A su vez, si introducimos las acciones antes de cmd.Run() no se habr\u00edan creado a\u00fan los_namespaces: es justo mientras transcurre en cmd.Run() cuando queremos modificar el contenedor. Por eso una opci\u00f3n es obligar al proceso a llamarse a una copia de s\u00ed mismo y cambiar el flujo del programa a la nueva funci\u00f3n child . Cabe destacar que el filesystem propuesto de Alpine no cuenta con Bash, as\u00ed que tendr\u00edamos que mandar ejecutar /bin/sh Por otro lado, es recomendable que a partir de ahora empezemos a manejar los errores que nos puedan aparecer: func must ( err error ) { if err != nil { panic ( err ) } } C\u00f3digo completo hasta ahora package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () case \"child\" : child () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getuid (), Size : 1 , }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } must ( cmd . Run ()) } func child () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) must ( syscall . Sethostname ([] byte ( \"container\" ))) must ( syscall . Chroot ( \"alpinefs/\" )) must ( os . Chdir ( \"/\" )) must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } func must ( err error ) { if err != nil { panic ( err ) } } \u00bfQU\u00c9 HEMOS CONSEGUIDO HASTA AHORA? En estos momentos hemos conseguido introducir unos cuantos namespaces , al menos los m\u00e1s significativos para realizar en este tutorial. El hostname namespace se puede comprobar de esta forma: # Fuera del contenedor root@bar:~$ hostname host root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ sethostname contenedor root@bar:~$ exit # Fuera del contenedor root@bar:~$ hostname host El user namespace lo hemos conseguido introducir a\u00f1adiendo los mapeos de usuario a root dentro del contenedor. Lo podemos comprobar de esta forma: # Fuera del contenedor root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ id uid=0(root) gid=0(root) groups=0(root) El mount namespace se puede comprobar de una forma muy sencilla: # Fuera del contenedor root@bar:~$ mount # ##### Aparecen muchos puntos de montaje usados por el host root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ mount proc on /proc type proc (rw,relatime) El pid namespace lo podemos comprobar realizando las siguientes instrucciones: # Fuera del contenedor root@bar:~$ go run contenedor.go run /bin/sh Corriendo '[/bin/sh]' con User ID 0 en PID 72724 # Dentro del contenedor root@bar:~$ ps a PID USER TIME COMMAND 1 root 0:00 /proc/self/exe child /bin/sh 5 root 0:00 /bin/sh 11 root 0:00 ps a","title":"Montamos nuestro propio /proc"},{"location":"tutoriales/contenedorgo/#anadiendo-cgroups-memoria-y-pid","text":"En este ejemplo a\u00f1adiremos un l\u00edmite al n\u00famero m\u00e1ximo de procesos en el cgroup (y, por lo tanto, en el contenedor) permitidos. Para ello necesitamos crear un nuevo directorio en /sys/fs/cgroup/pids/ . Al crear el directorio autom\u00e1ticamente el sistema a\u00f1ade los archivos necesarios para mostrar los datos del nuevo Cgroup y para modificar los l\u00edmites que se le quieran a\u00f1adir. En nuestro caso el grupo se llamar\u00e1 demo . Para modificar el n\u00famero m\u00e1ximo de procesos que se permite en el contenedor s\u00f3lo es necesario modificar el archivo donde se indica el n\u00famero (pondremos como m\u00e1ximo 12 procesos) y otro donde se introduce al proceso del contenedor en el grupo de control. Adem\u00e1s a\u00f1adiremos un n\u00famero m\u00e1ximo de bytes de memoria que se le asignan al contenedor, aunque esto es m\u00e1s complicado de comprobar que funciona correctamente, pero los pasos son los mismos que en el anterior caso. func cg () cgroups := \"/sys/fs/cgroup\" // Creando cgroup para PIDs pids := filepath . Join ( cgroups , \"pids/demo\" ) if _ , err := os . Stat ( pids ); os . IsNotExist ( err ) { must ( os . Mkdir ( pids , 0755 )) } // Creando cgroup para PIDs memory := filepath . Join ( cgroups , \"memory/demo\" ) if _ , err := os . Stat ( memory ); os . IsnotExist ( err ) { must ( os . Mkdir ( memory , 0755 )) } //Establecemos limite y metemos al proceso dentro del grupo de procesos must ( ioutil . WriteFile ( filepath . Join ( pids , \"pids.max\" ), [] byte ( \"10\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( pids , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"memory.limit_in_bytes\" ), [] byte ( \"2M\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) } S\u00f3lo hace falta llamar a esta funci\u00f3n desde el principio de la funci\u00f3n child .","title":"A\u00f1adiendo Cgroups (memoria y PID)"},{"location":"tutoriales/contenedorgo/#anexo-mejora-con-pivot_root","text":"Cuando se introdujo en el tutorial la llamada al sistema chroot se mencion\u00f3 la posibilidad de utilizar otra m\u00e1s segura: pivot_root . Aunque antiguamente, en los primeros contenedores, se utilizaba la primera opci\u00f3n, se ha llegado a la conclusi\u00f3n de que tiene varios fallos de seguridad que permiten \"salir o escapar\" del contenedor. pivot_root aprovecha el mount namespace ya que permite hacer unmount del antiguo root y no lo hace accesible en el namespace del contenedor. Si usamos \u00fanicamente chroot podemos acceder al Host con el siguiente comando: chroot /proc/1/root . Lo que hay que saber para poder usar pivot_root es que necesita dos argumentos, el primero es la direcci\u00f3n del nuevo directorio ra\u00edz (no viene en la documentaci\u00f3n pero debe estar montado sobre s\u00ed mismo con la opci\u00f3n bind ) y el segundo es la direcci\u00f3n donde se va a situar el antiguo directorio ra\u00edz. El c\u00f3digo completo del tutorial quedar\u00eda as\u00ed: C\u00d3DIGO COMPLETO package main import ( \"fmt\" \"os\" \"os/exec\" \"syscall\" \"io/ioutil\" \"strconv\" \"path/filepath\" ) func main () { switch os . Args [ 1 ] { case \"run\" : run () case \"child\" : child () default : panic ( \"\u00bfArgumento Invalido?\" ) } } func run () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cmd := exec . Command ( \"/proc/self/exe\" , append ([] string { \"child\" }, os . Args [ 2 :] ... ) ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr cmd . SysProcAttr = & syscall . SysProcAttr { Cloneflags : syscall . CLONE_NEWUTS | syscall . CLONE_NEWUSER | syscall . CLONE_NEWNS | syscall . CLONE_NEWPID , UidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getuid (), Size : 1 , }, }, GidMappings : [] syscall . SysProcIDMap { { ContainerID : 0 , HostID : os . Getgid (), Size : 1 , }, }, } must ( cmd . Run ()) } func child () { fmt . Printf ( \"Corriendo '%v' con User ID %d en PID %d \\n\" , os . Args [ 2 :], os . Getuid (), os . Getpid ()) cg () must ( syscall . Sethostname ([] byte ( \"container\" ))) pivot () must ( syscall . Mount ( \"proc\" , \"proc\" , \"proc\" , 0 , \"\" )) cmd := exec . Command ( os . Args [ 2 ], os . Args [ 3 :] ... ) cmd . Stdin = os . Stdin cmd . Stdout = os . Stdout cmd . Stderr = os . Stderr must ( syscall . Unmount ( \".old_root\" , syscall . MNT_DETACH )) must ( os . Remove ( \".old_root\" )) defer func () { must ( syscall . Unmount ( \"proc\" , 0 )) }() must ( cmd . Run ()) } func cg () { cgroups := \"/sys/fs/cgroup\" pids := filepath . Join ( cgroups , \"pids/demo\" ) if _ , err := os . Stat ( pids ); os . IsNotExist ( err ) { must ( os . Mkdir ( pids , 0755 )) } memory := filepath . Join ( cgroups , \"memory/demo\" ) if _ , err := os . Stat ( memory ); os . IsNotExist ( err ) { must ( os . Mkdir ( memory , 0755 )) } must ( ioutil . WriteFile ( filepath . Join ( pids , \"pids.max\" ), [] byte ( \"22\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( pids , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"memory.limit_in_bytes\" ), [] byte ( \"2M\" ), 0700 )) must ( ioutil . WriteFile ( filepath . Join ( memory , \"cgroup.procs\" ), [] byte ( strconv . Itoa ( os . Getpid ())), 0700 )) } func pivot () { must ( syscall . Mount ( \"alpinefs\" , \"alpinefs\" , \"\" , syscall . MS_BIND | syscall . MS_REC , \"\" )) if _ , err := os . Stat ( \"alpinefs/.old_root\" ); os . IsNotExist ( err ) { must ( os . Mkdir ( \"alpinefs/.old_root\" , 0700 )) } must ( syscall . PivotRoot ( \"alpinefs\" , \"alpinefs/.old_root\" )) must ( os . Chdir ( \"/\" )) } func must ( err error ) { if err != nil { panic ( err ) } }","title":"Anexo: mejora con pivot_root"}]}